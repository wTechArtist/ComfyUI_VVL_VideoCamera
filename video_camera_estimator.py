import torch
import numpy as np
import json
import os
import tempfile
import cv2
import subprocess
import shutil
from PIL import Image
from typing import List, Dict, Any

# æ·»åŠ ComfyUIç±»å‹å¯¼å…¥
try:
    from comfy.comfy_types import IO
except ImportError:
    # å¦‚æœæ— æ³•å¯¼å…¥ï¼Œåˆ›å»ºä¸€ä¸ªå…¼å®¹çš„ç±»
    class IO:
        VIDEO = "VIDEO"

# ä¿®å¤å¯¼å…¥è·¯å¾„
try:
    from .utils.camera_utils import CameraParameterEstimator
except ImportError:
    try:
        # å°è¯•ç›¸å¯¹å¯¼å…¥
        import sys
        current_dir = os.path.dirname(os.path.abspath(__file__))
        utils_path = os.path.join(current_dir, 'utils')
        if utils_path not in sys.path:
            sys.path.insert(0, utils_path)
        from camera_utils import CameraParameterEstimator
    except ImportError:
        # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ
        import sys
        current_dir = os.path.dirname(os.path.abspath(__file__))
        sys.path.insert(0, current_dir)
        try:
            from utils.camera_utils import CameraParameterEstimator
        except ImportError as e:
            print(f"æ— æ³•å¯¼å…¥ CameraParameterEstimator: {e}")
            # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿç±»ï¼Œé¿å…å¯åŠ¨å¤±è´¥
            class CameraParameterEstimator:
                def __init__(self):
                    self.error = "CameraParameterEstimator å¯¼å…¥å¤±è´¥"
                
                def estimate_from_video(self, *args, **kwargs):
                    return {
                        "success": False,
                        "error": self.error,
                        "intrinsics": None,
                        "poses": [],
                        "statistics": None,
                        "frame_count": 0,
                        "trajectory_visualization": None
                    }

# å…¨å±€ä¼°è®¡å™¨å®ä¾‹
_ESTIMATOR = None

class VideoCameraEstimator:
    """è§†é¢‘ç›¸æœºå‚æ•°ä¼°è®¡èŠ‚ç‚¹"""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "video": (IO.VIDEO, {
                    "tooltip": "ä»LoadVideoèŠ‚ç‚¹è¾“å…¥çš„è§†é¢‘ï¼Œæˆ–è€…ç›´æ¥è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„"
                }),
            },
            "optional": {
                "video_path": ("STRING", {
                    "default": "",
                    "tooltip": "è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œå¦‚æœvideoè¾“å…¥ä¸ºç©ºåˆ™ä½¿ç”¨æ­¤è·¯å¾„ï¼‰"
                }),
                "frame_interval": ("INT", {
                    "default": 10,
                    "min": 1,
                    "max": 100,
                    "step": 1,
                    "tooltip": "æå–å¸§çš„é—´éš”ï¼Œæ•°å€¼è¶Šå°æå–çš„å¸§è¶Šå¤š"
                }),
                "max_frames": ("INT", {
                    "default": 50,
                    "min": 5,
                    "max": 200,
                    "step": 5,
                    "tooltip": "æœ€å¤§æå–å¸§æ•°ï¼Œç”¨äºæ§åˆ¶è®¡ç®—é‡"
                }),
                "estimation_method": (["colmap","opencv_sfm", "feature_matching", "hybrid" ], {
                    "default": "colmap",
                    "tooltip": "ç›¸æœºå‚æ•°ä¼°è®¡æ–¹æ³•ï¼Œcolmapæä¾›æœ€é«˜ç²¾åº¦"
                }),
                "enable_visualization": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "æ˜¯å¦ç”Ÿæˆè½¨è¿¹å¯è§†åŒ–å›¾åƒ"
                }),
                "output_format": (["json", "detailed_json"], {
                    "default": "detailed_json",
                    "tooltip": "è¾“å‡ºæ ¼å¼ï¼Œè¯¦ç»†æ¨¡å¼åŒ…å«æ›´å¤šç»Ÿè®¡ä¿¡æ¯"
                }),
                # COLMAP ç‰¹å®šå‚æ•°
                "colmap_feature_type": (["sift", "superpoint", "disk"], {
                    "default": "sift",
                    "tooltip": "COLMAPç‰¹å¾æ£€æµ‹å™¨ç±»å‹ï¼ŒSIFTæœ€ç¨³å®šï¼ŒSuperPointå’ŒDISKæ›´ç°ä»£"
                }),
                "colmap_matcher_type": (["exhaustive", "sequential", "spatial"], {
                    "default": "sequential",
                    "tooltip": "COLMAPåŒ¹é…ç­–ç•¥ï¼Œsequentialé€‚åˆè§†é¢‘åºåˆ—"
                }),
                "colmap_quality": (["low", "medium", "high", "extreme"], {
                    "default": "medium",
                    "tooltip": "COLMAPé‡å»ºè´¨é‡ï¼Œhigherè´¨é‡éœ€è¦æ›´å¤šæ—¶é—´"
                }),
                "enable_dense_reconstruction": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "æ˜¯å¦å¯ç”¨å¯†é›†é‡å»ºï¼ˆä»…é™COLMAPï¼Œéœ€è¦æ›´å¤šè®¡ç®—èµ„æºï¼‰"
                })
            }
        }

    RETURN_TYPES = ("STRING", "IMAGE", "STRING", "STRING", "STRING")
    RETURN_NAMES = ("intrinsics_json", "trajectory_visualization", "poses_json", "statistics_json", "point_cloud_info")
    FUNCTION = "estimate_camera_parameters"
    CATEGORY = "ğŸ’ƒVVL/VideoCamera"

    def __init__(self):
        global _ESTIMATOR
        if _ESTIMATOR is None:
            _ESTIMATOR = CameraParameterEstimator()
        self.estimator = _ESTIMATOR
        # æ£€æŸ¥COLMAPæ˜¯å¦å¯ç”¨
        self.colmap_available = self._check_colmap_availability()

    def estimate_camera_parameters(self, video, video_path: str = "", frame_interval: int = 10, max_frames: int = 50,
                                 estimation_method: str = "hybrid", enable_visualization: bool = True,
                                 output_format: str = "detailed_json", colmap_feature_type: str = "sift",
                                 colmap_matcher_type: str = "sequential", colmap_quality: str = "medium",
                                 enable_dense_reconstruction: bool = False) -> tuple:
        """
        ä»è§†é¢‘ä¼°è®¡ç›¸æœºå‚æ•°çš„ä¸»å‡½æ•°
        
        Args:
            video: æ¥è‡ªLoadVideoèŠ‚ç‚¹çš„è§†é¢‘å¯¹è±¡æˆ–None
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆå¤‡ç”¨ï¼‰
            frame_interval: å¸§æå–é—´éš”
            max_frames: æœ€å¤§å¸§æ•°
            estimation_method: ä¼°è®¡æ–¹æ³•
            enable_visualization: æ˜¯å¦ç”Ÿæˆå¯è§†åŒ–
            output_format: è¾“å‡ºæ ¼å¼
            colmap_feature_type: COLMAPç‰¹å¾ç±»å‹
            colmap_matcher_type: COLMAPåŒ¹é…ç­–ç•¥
            colmap_quality: COLMAPé‡å»ºè´¨é‡
            enable_dense_reconstruction: æ˜¯å¦å¯ç”¨å¯†é›†é‡å»º
            
        Returns:
            tuple: (å†…å‚JSON, è½¨è¿¹å¯è§†åŒ–å›¾åƒ, ä½å§¿JSON, ç»Ÿè®¡ä¿¡æ¯JSON, ç‚¹äº‘ä¿¡æ¯JSON)
        """
        
        try:
            # ç¡®å®šè§†é¢‘æ–‡ä»¶è·¯å¾„
            actual_video_path = None
            
            # ä¼˜å…ˆä½¿ç”¨videoè¾“å…¥ï¼ˆæ¥è‡ªLoadVideoèŠ‚ç‚¹ï¼‰
            if video is not None:
                # å¦‚æœvideoæ˜¯VideoFromFileå¯¹è±¡ï¼Œè·å–å…¶æ–‡ä»¶è·¯å¾„
                if hasattr(video, '_VideoFromFile__file'):
                    # è®¿é—®ç§æœ‰å±æ€§ __file
                    file_attr = video._VideoFromFile__file
                    if isinstance(file_attr, str):
                        actual_video_path = file_attr
                    else:
                        print(f"video.__file ä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹: {type(file_attr)}")
                elif hasattr(video, 'path'):
                    actual_video_path = video.path
                elif hasattr(video, 'video_path'):
                    actual_video_path = video.video_path
                elif hasattr(video, '_path'):
                    actual_video_path = video._path
                elif hasattr(video, 'file_path'):
                    actual_video_path = video.file_path
                else:
                    # å°è¯•ç›´æ¥ä½¿ç”¨videoä½œä¸ºè·¯å¾„
                    if isinstance(video, str):
                        actual_video_path = video
                    else:
                        print(f"æ— æ³•ä»videoå¯¹è±¡è·å–è·¯å¾„ï¼Œvideoç±»å‹: {type(video)}")
                        print(f"videoå¯¹è±¡å±æ€§: {[attr for attr in dir(video) if not attr.startswith('_')]}")
                        if hasattr(video, '__dict__'):
                            print(f"videoå¯¹è±¡å†…å®¹: {video.__dict__}")
                        
                        # å°è¯•è°ƒç”¨get_componentsæ¥çœ‹æ˜¯å¦èƒ½è·å–ä¿¡æ¯
                        try:
                            if hasattr(video, 'get_components'):
                                components = video.get_components()
                                print(f"video components: {components}")
                        except Exception as e:
                            print(f"è°ƒç”¨get_componentså¤±è´¥: {e}")
            
            # å¦‚æœvideoè¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨video_path
            if actual_video_path is None or not actual_video_path:
                if video_path and video_path.strip():
                    actual_video_path = video_path.strip()
                else:
                    raise ValueError("æœªæä¾›æœ‰æ•ˆçš„è§†é¢‘è¾“å…¥ã€‚è¯·è¿æ¥LoadVideoèŠ‚ç‚¹åˆ°videoè¾“å…¥ï¼Œæˆ–åœ¨video_pathä¸­è¾“å…¥æ–‡ä»¶è·¯å¾„ã€‚")
            
            # éªŒè¯è§†é¢‘æ–‡ä»¶
            if not os.path.exists(actual_video_path):
                raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {actual_video_path}")
            
            # éªŒè¯è§†é¢‘æ ¼å¼
            if not self._is_valid_video_format(actual_video_path):
                raise ValueError("ä¸æ”¯æŒçš„è§†é¢‘æ ¼å¼")
            
            print(f"å¼€å§‹å¤„ç†è§†é¢‘: {actual_video_path}")
            print(f"å‚æ•° - å¸§é—´éš”: {frame_interval}, æœ€å¤§å¸§æ•°: {max_frames}, æ–¹æ³•: {estimation_method}")
            
            # æ ¹æ®ä¼°è®¡æ–¹æ³•é€‰æ‹©å¤„ç†æµç¨‹
            if estimation_method == "colmap":
                if not self.colmap_available:
                    raise RuntimeError("COLMAP æœªå®‰è£…æˆ–ä¸å¯ç”¨ï¼Œè¯·å®‰è£… COLMAP æˆ–é€‰æ‹©å…¶ä»–ä¼°è®¡æ–¹æ³•")
                
                result = self._estimate_with_colmap(
                    video_path=actual_video_path,
                    frame_interval=frame_interval,
                    max_frames=max_frames,
                    feature_type=colmap_feature_type,
                    matcher_type=colmap_matcher_type,
                    quality=colmap_quality,
                    enable_dense=enable_dense_reconstruction
                )
            else:
                # ä½¿ç”¨åŸæœ‰çš„ä¼°è®¡å™¨
                result = self.estimator.estimate_from_video(
                    video_path=actual_video_path,
                    frame_interval=frame_interval,
                    max_frames=max_frames
                )
            
            if not result["success"]:
                raise RuntimeError(f"ç›¸æœºå‚æ•°ä¼°è®¡å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
            # å‡†å¤‡è¾“å‡º
            intrinsics_json = self._format_intrinsics_output(result["intrinsics"], output_format)
            poses_json = self._format_poses_output(result["poses"], output_format)
            statistics_json = self._format_statistics_output(result["statistics"], result, output_format)
            
            # å¤„ç†ç‚¹äº‘ä¿¡æ¯ï¼ˆCOLMAPç‰¹æœ‰ï¼‰
            point_cloud_info = self._format_point_cloud_output(result.get("point_cloud", {}), output_format)
            
            # å¤„ç†å¯è§†åŒ–å›¾åƒ
            if enable_visualization and result.get("trajectory_visualization") is not None:
                trajectory_img = self._prepare_visualization_image(result["trajectory_visualization"])
            else:
                trajectory_img = self._create_empty_visualization()
            
            print(f"æˆåŠŸå¤„ç† {result['frame_count']} å¸§")
            if result['intrinsics']:
                print(f"ä¼°è®¡çš„ç„¦è·: {result['intrinsics']['focal_length']:.2f}")
            
            return (intrinsics_json, trajectory_img, poses_json, statistics_json, point_cloud_info)
            
        except Exception as e:
            error_msg = f"è§†é¢‘ç›¸æœºå‚æ•°ä¼°è®¡å‡ºé”™: {str(e)}"
            print(error_msg)
            
            # è¿”å›é”™è¯¯ä¿¡æ¯
            error_json = json.dumps({"error": error_msg, "success": False}, ensure_ascii=False, indent=2)
            empty_img = self._create_empty_visualization()
            
            return (error_json, empty_img, error_json, error_json, error_json)

    def _check_colmap_availability(self) -> bool:
        """æ£€æŸ¥COLMAPæ˜¯å¦å¯ç”¨"""
        try:
            # ä¼˜å…ˆæ£€æŸ¥PyColmap
            import pycolmap
            print("æ£€æµ‹åˆ° PyColmapï¼Œå°†ä½¿ç”¨ Python API è¿›è¡Œ COLMAP é‡å»º")
            return True
        except ImportError:
            try:
                # å°è¯•å‘½ä»¤è¡Œç‰ˆæœ¬
                result = subprocess.run(['colmap', '--help'], 
                                      capture_output=True, text=True, timeout=10)
                if result.returncode == 0:
                    print("æ£€æµ‹åˆ°å‘½ä»¤è¡Œ COLMAP")
                    return True
            except (subprocess.TimeoutExpired, FileNotFoundError, subprocess.SubprocessError):
                pass
        print("COLMAP ä¸å¯ç”¨ï¼šéœ€è¦å®‰è£… pycolmap æˆ–å‘½ä»¤è¡Œç‰ˆæœ¬çš„ COLMAP")
        return False

    def _estimate_with_colmap(self, video_path: str, frame_interval: int, max_frames: int,
                            feature_type: str, matcher_type: str, quality: str, enable_dense: bool) -> Dict:
        """ä½¿ç”¨COLMAPè¿›è¡Œç›¸æœºå‚æ•°ä¼°è®¡"""
        
        # ä¼˜å…ˆä½¿ç”¨PyColmap
        try:
            import pycolmap
            return self._estimate_with_pycolmap(
                video_path, frame_interval, max_frames, 
                feature_type, matcher_type, quality, enable_dense
            )
        except ImportError:
            # å›é€€åˆ°å‘½ä»¤è¡Œç‰ˆæœ¬
            return self._estimate_with_colmap_cli(
                video_path, frame_interval, max_frames, 
                feature_type, matcher_type, quality, enable_dense
            )

    def _estimate_with_pycolmap(self, video_path: str, frame_interval: int, max_frames: int,
                              feature_type: str, matcher_type: str, quality: str, enable_dense: bool) -> Dict:
        """ä½¿ç”¨PyColmapè¿›è¡Œç›¸æœºå‚æ•°ä¼°è®¡"""
        import pycolmap
        
        # åˆ›å»ºä¸´æ—¶å·¥ä½œç›®å½•
        temp_dir = tempfile.mkdtemp(prefix="pycolmap_estimation_")
        images_dir = os.path.join(temp_dir, "images")
        database_path = os.path.join(temp_dir, "database.db")
        
        try:
            os.makedirs(images_dir, exist_ok=True)
            
            # 1. æå–è§†é¢‘å¸§
            print("æå–è§†é¢‘å¸§...")
            frame_paths = self._extract_frames_for_colmap(video_path, images_dir, frame_interval, max_frames)
            
            if len(frame_paths) < 3:
                raise ValueError("æå–çš„å¸§æ•°è¿‡å°‘ï¼ˆ< 3ï¼‰ï¼Œæ— æ³•è¿›è¡Œé‡å»º")
            
            print(f"æˆåŠŸæå– {len(frame_paths)} å¸§")
            
            # 2. è®¾ç½®è´¨é‡å‚æ•°
            quality_settings = {
                "low": {"max_image_size": 800, "max_num_features": 4096},
                "medium": {"max_image_size": 1200, "max_num_features": 8192},
                "high": {"max_image_size": 1600, "max_num_features": 16384},
                "extreme": {"max_image_size": 2400, "max_num_features": 32768}
            }
            settings = quality_settings[quality]
            
            # 3. ä½¿ç”¨PyColmapçš„ç®€åŒ–API
            print("å¼€å§‹ PyColmap é‡å»º...")
            
            # åˆ›å»ºæ•°æ®åº“
            database = pycolmap.Database()
            database.create(database_path)
            
            # ç‰¹å¾æå–é€‰é¡¹
            sift_options = pycolmap.SiftExtractionOptions()
            sift_options.max_image_size = settings["max_image_size"]
            sift_options.max_num_features = settings["max_num_features"]
            
            # ç‰¹å¾æå–
            print("ç‰¹å¾æå–...")
            pycolmap.extract_features(
                database_path=database_path,
                image_path=images_dir,
                sift_options=sift_options
            )
            
            # ç‰¹å¾åŒ¹é…
            print("ç‰¹å¾åŒ¹é…...")
            if matcher_type == "exhaustive":
                match_options = pycolmap.ExhaustiveMatchingOptions()
                pycolmap.match_exhaustive(
                    database_path=database_path,
                    match_options=match_options
                )
            elif matcher_type == "sequential":
                match_options = pycolmap.SequentialMatchingOptions()
                match_options.overlap = 10
                pycolmap.match_sequential(
                    database_path=database_path,
                    match_options=match_options
                )
            else:  # spatial
                match_options = pycolmap.SpatialMatchingOptions()
                pycolmap.match_spatial(
                    database_path=database_path,
                    match_options=match_options
                )
            
            # å¢é‡é‡å»º
            print("å¢é‡é‡å»º...")
            output_path = os.path.join(temp_dir, "reconstruction")
            os.makedirs(output_path, exist_ok=True)
            
            # ä½¿ç”¨é»˜è®¤çš„é‡å»ºé€‰é¡¹
            mapper_options = pycolmap.IncrementalMapperOptions()
            
            # åªè®¾ç½®å­˜åœ¨çš„å±æ€§
            try:
                mapper_options.ba_refine_focal_length = True
                mapper_options.ba_refine_principal_point = True
            except AttributeError:
                pass  # å¦‚æœå±æ€§ä¸å­˜åœ¨ï¼Œè·³è¿‡
            
            try:
                mapper_options.init_min_num_inliers = 100
            except AttributeError:
                pass
            
            try:
                mapper_options.init_max_reg_trials = 2
            except AttributeError:
                pass
            
            # æ‰§è¡Œé‡å»º
            reconstruction = pycolmap.incremental_mapping(
                database_path=database_path,
                image_path=images_dir,
                output_path=output_path,
                options=mapper_options
            )
            
            if not reconstruction:
                raise RuntimeError("PyColmap é‡å»ºå¤±è´¥ï¼šæ²¡æœ‰ç”Ÿæˆé‡å»ºç»“æœ")
            
            # è·å–é‡å»ºç»“æœ
            if isinstance(reconstruction, dict):
                # å¦‚æœè¿”å›çš„æ˜¯å­—å…¸ï¼Œå°è¯•è·å–ç¬¬ä¸€ä¸ªé‡å»º
                if len(reconstruction) == 0:
                    raise RuntimeError("PyColmap é‡å»ºå¤±è´¥ï¼šé‡å»ºç»“æœä¸ºç©º")
                recon = list(reconstruction.values())[0]
            elif isinstance(reconstruction, list):
                if len(reconstruction) == 0:
                    raise RuntimeError("PyColmap é‡å»ºå¤±è´¥ï¼šé‡å»ºç»“æœä¸ºç©º")
                recon = reconstruction[0]
            else:
                recon = reconstruction
            
            print(f"é‡å»ºæˆåŠŸï¼š{len(recon.cameras)} ä¸ªç›¸æœºï¼Œ{len(recon.images)} å¼ å›¾åƒï¼Œ{len(recon.points3D)} ä¸ª3Dç‚¹")
            
            # è§£æç»“æœ
            result = self._parse_with_pycolmap(recon, frame_paths, {})
            
            # ç”Ÿæˆå¯è§†åŒ–
            if result["success"]:
                result["trajectory_visualization"] = self._create_colmap_visualization(result["poses"])
            
            return result
            
        except Exception as e:
            print(f"PyColmap ä¼°è®¡è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            
            # å°è¯•ä½¿ç”¨æ›´ç®€å•çš„æ–¹æ³•
            try:
                print("å°è¯•ä½¿ç”¨ç®€åŒ–çš„ PyColmap æ–¹æ³•...")
                result = self._simple_pycolmap_reconstruction(images_dir, database_path, frame_paths)
                return result
            except Exception as e2:
                print(f"ç®€åŒ–æ–¹æ³•ä¹Ÿå¤±è´¥: {e2}")
                return {
                    "success": False,
                    "error": f"PyColmap é‡å»ºå¤±è´¥: {str(e)}",
                    "intrinsics": None,
                    "poses": [],
                    "statistics": {"total_distance": 0, "average_speed": 0},
                    "frame_count": 0,
                    "trajectory_visualization": None,
                    "point_cloud": {}
                }
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                shutil.rmtree(temp_dir)
            except:
                pass

    def _simple_pycolmap_reconstruction(self, images_dir: str, database_path: str, frame_paths: List[str]) -> Dict:
        """ä½¿ç”¨æœ€ç®€å•çš„PyColmapé‡å»ºæ–¹æ³•"""
        import pycolmap
        
        try:
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = os.path.dirname(database_path)
            reconstruction_dir = os.path.join(output_dir, "simple_recon")
            os.makedirs(reconstruction_dir, exist_ok=True)
            
            # ä½¿ç”¨æœ€åŸºæœ¬çš„ç‰¹å¾æå–å’ŒåŒ¹é…
            print("ç®€åŒ–ç‰¹å¾æå–...")
            pycolmap.extract_features(database_path, images_dir)
            
            print("ç®€åŒ–ç‰¹å¾åŒ¹é…...")
            pycolmap.match_exhaustive(database_path)
            
            print("ç®€åŒ–é‡å»º...")
            reconstructions = pycolmap.incremental_mapping(database_path, images_dir, reconstruction_dir)
            
            if not reconstructions:
                raise RuntimeError("ç®€åŒ–é‡å»ºå¤±è´¥")
            
            # è·å–ç¬¬ä¸€ä¸ªé‡å»º
            if isinstance(reconstructions, dict) and len(reconstructions) > 0:
                recon = list(reconstructions.values())[0]
            elif isinstance(reconstructions, list) and len(reconstructions) > 0:
                recon = reconstructions[0]
            else:
                recon = reconstructions
            
            print(f"ç®€åŒ–é‡å»ºæˆåŠŸï¼š{len(recon.cameras)} ä¸ªç›¸æœºï¼Œ{len(recon.images)} å¼ å›¾åƒ")
            
            # è§£æç»“æœ
            result = self._parse_with_pycolmap(recon, frame_paths, {})
            
            # ç”Ÿæˆå¯è§†åŒ–
            if result["success"]:
                result["trajectory_visualization"] = self._create_colmap_visualization(result["poses"])
            
            return result
            
        except Exception as e:
            print(f"ç®€åŒ–é‡å»ºå¤±è´¥: {e}")
            return {
                "success": False,
                "error": f"ç®€åŒ–PyColmapé‡å»ºå¤±è´¥: {str(e)}",
                "intrinsics": None,
                "poses": [],
                "statistics": {"total_distance": 0, "average_speed": 0},
                "frame_count": 0,
                "trajectory_visualization": None,
                "point_cloud": {}
            }

    def _extract_frames_for_colmap(self, video_path: str, output_dir: str, interval: int, max_frames: int) -> List[str]:
        """ä¸ºCOLMAPæå–è§†é¢‘å¸§"""
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        frame_paths = []
        frame_count = 0
        current_frame = 0
        
        while current_frame < total_frames and frame_count < max_frames:
            cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame)
            ret, frame = cap.read()
            
            if ret:
                # ä¿å­˜å¸§
                frame_filename = f"frame_{frame_count:06d}.jpg"
                frame_path = os.path.join(output_dir, frame_filename)
                cv2.imwrite(frame_path, frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
                frame_paths.append(frame_path)
                frame_count += 1
            
            current_frame += interval
        
        cap.release()
        return frame_paths

    def _run_colmap_feature_extraction(self, database_path: str, images_dir: str, feature_type: str, quality: str):
        """è¿è¡ŒCOLMAPç‰¹å¾æå–"""
        
        # è®¾ç½®è´¨é‡å‚æ•°
        quality_settings = {
            "low": {"max_image_size": 800, "max_num_features": 4096},
            "medium": {"max_image_size": 1200, "max_num_features": 8192},
            "high": {"max_image_size": 1600, "max_num_features": 16384},
            "extreme": {"max_image_size": 2400, "max_num_features": 32768}
        }
        settings = quality_settings[quality]
        
        cmd = [
            "colmap", "feature_extractor",
            "--database_path", database_path,
            "--image_path", images_dir,
            "--ImageReader.single_camera", "1",
            "--ImageReader.camera_model", "PINHOLE",
            "--SiftExtraction.max_image_size", str(settings["max_image_size"]),
            "--SiftExtraction.max_num_features", str(settings["max_num_features"])
        ]
        
        if feature_type != "sift":
            # å¯¹äºéSIFTç‰¹å¾ï¼Œå¯èƒ½éœ€è¦é¢å¤–é…ç½®
            print(f"æ³¨æ„ï¼š{feature_type} ç‰¹å¾å¯èƒ½éœ€è¦é¢å¤–é…ç½®ï¼Œå½“å‰ä½¿ç”¨SIFTä½œä¸ºåå¤‡")
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
        if result.returncode != 0:
            raise RuntimeError(f"COLMAPç‰¹å¾æå–å¤±è´¥: {result.stderr}")

    def _run_colmap_matching(self, database_path: str, matcher_type: str):
        """è¿è¡ŒCOLMAPç‰¹å¾åŒ¹é…"""
        
        if matcher_type == "exhaustive":
            cmd = ["colmap", "exhaustive_matcher", "--database_path", database_path]
        elif matcher_type == "sequential":
            cmd = ["colmap", "sequential_matcher", "--database_path", database_path,
                   "--SequentialMatching.overlap", "10"]
        elif matcher_type == "spatial":
            cmd = ["colmap", "spatial_matcher", "--database_path", database_path]
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åŒ¹é…å™¨ç±»å‹: {matcher_type}")
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)
        if result.returncode != 0:
            raise RuntimeError(f"COLMAPç‰¹å¾åŒ¹é…å¤±è´¥: {result.stderr}")

    def _run_colmap_sparse_reconstruction(self, database_path: str, images_dir: str, output_dir: str):
        """è¿è¡ŒCOLMAPç¨€ç–é‡å»º"""
        
        cmd = [
            "colmap", "mapper",
            "--database_path", database_path,
            "--image_path", images_dir,
            "--output_path", output_dir
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=1800)
        if result.returncode != 0:
            raise RuntimeError(f"COLMAPç¨€ç–é‡å»ºå¤±è´¥: {result.stderr}")

    def _run_colmap_dense_reconstruction(self, images_dir: str, sparse_dir: str, dense_dir: str) -> Dict:
        """è¿è¡ŒCOLMAPå¯†é›†é‡å»º"""
        dense_info = {"enabled": True}
        
        try:
            # å›¾åƒå»ç•¸å˜
            cmd = ["colmap", "image_undistorter",
                   "--image_path", images_dir,
                   "--input_path", sparse_dir,
                   "--output_path", dense_dir,
                   "--output_type", "COLMAP"]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)
            if result.returncode != 0:
                dense_info["undistort_error"] = result.stderr
                return dense_info
            
            # ç«‹ä½“åŒ¹é…
            stereo_dir = os.path.join(dense_dir, "stereo")
            cmd = ["colmap", "patch_match_stereo",
                   "--workspace_path", dense_dir,
                   "--workspace_format", "COLMAP",
                   "--PatchMatchStereo.geom_consistency", "true"]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=1800)
            if result.returncode != 0:
                dense_info["stereo_error"] = result.stderr
                return dense_info
            
            # ç«‹ä½“èåˆ
            cmd = ["colmap", "stereo_fusion",
                   "--workspace_path", dense_dir,
                   "--workspace_format", "COLMAP",
                   "--input_type", "geometric",
                   "--output_path", os.path.join(dense_dir, "fused.ply")]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)
            if result.returncode != 0:
                dense_info["fusion_error"] = result.stderr
            else:
                dense_info["point_cloud_path"] = os.path.join(dense_dir, "fused.ply")
                
        except Exception as e:
            dense_info["error"] = str(e)
        
        return dense_info

    def _parse_colmap_results(self, reconstruction_dir: str, frame_paths: List[str], dense_info: Dict) -> Dict:
        """è§£æCOLMAPé‡å»ºç»“æœ"""
        
        try:
            # å°è¯•ä½¿ç”¨pycolmapè¯»å–ç»“æœ
            try:
                import pycolmap
                reconstruction = pycolmap.Reconstruction(reconstruction_dir)
                return self._parse_with_pycolmap(reconstruction, frame_paths, dense_info)
            except ImportError:
                # ä½¿ç”¨æ–‡æœ¬æ–‡ä»¶è§£æ
                return self._parse_colmap_text_files(reconstruction_dir, frame_paths, dense_info)
                
        except Exception as e:
            return {
                "success": False,
                "error": f"è§£æCOLMAPç»“æœå¤±è´¥: {str(e)}",
                "intrinsics": None,
                "poses": [],
                "statistics": {"total_distance": 0, "average_speed": 0},
                "frame_count": 0,
                "trajectory_visualization": None,
                "point_cloud": dense_info
            }

    def _parse_with_pycolmap(self, reconstruction, frame_paths: List[str], dense_info: Dict) -> Dict:
        """ä½¿ç”¨pycolmapè§£æç»“æœ"""
        
        if len(reconstruction.cameras) == 0 or len(reconstruction.images) == 0:
            raise ValueError("é‡å»ºå¤±è´¥ï¼šæ²¡æœ‰æ‰¾åˆ°ç›¸æœºæˆ–å›¾åƒ")
        
        # è·å–ç›¸æœºå†…å‚
        camera = list(reconstruction.cameras.values())[0]
        
        # å®‰å…¨åœ°è·å–ç›¸æœºå‚æ•°
        try:
            # ä¸åŒç‰ˆæœ¬çš„PyColmapå¯èƒ½æœ‰ä¸åŒçš„å±æ€§å
            focal_length = None
            focal_length_y = None
            principal_point = [0, 0]
            image_size = [0, 0]
            camera_model = "UNKNOWN"
            distortion = []
            
            # å°è¯•è·å–ç„¦è·
            if hasattr(camera, 'params') and len(camera.params) > 0:
                focal_length = float(camera.params[0])
                if len(camera.params) > 1:
                    focal_length_y = float(camera.params[1])
                else:
                    focal_length_y = focal_length
                
                # å°è¯•è·å–ä¸»ç‚¹
                if len(camera.params) > 3:
                    principal_point = [float(camera.params[2]), float(camera.params[3])]
                
                # ç•¸å˜å‚æ•°
                if len(camera.params) > 4:
                    distortion = [float(p) for p in camera.params[4:]]
            
            # å°è¯•è·å–å›¾åƒå°ºå¯¸
            if hasattr(camera, 'width') and hasattr(camera, 'height'):
                image_size = [camera.width, camera.height]
                # å¦‚æœæ²¡æœ‰ä¸»ç‚¹ï¼Œä½¿ç”¨å›¾åƒä¸­å¿ƒ
                if principal_point == [0, 0]:
                    principal_point = [camera.width / 2, camera.height / 2]
            
            # å°è¯•è·å–ç›¸æœºæ¨¡å‹åç§°
            if hasattr(camera, 'model_name'):
                camera_model = camera.model_name
            elif hasattr(camera, 'model'):
                if hasattr(camera.model, 'name'):
                    camera_model = camera.model.name
                else:
                    camera_model = str(camera.model)
            elif hasattr(camera, 'model_id'):
                # å°†æ¨¡å‹IDæ˜ å°„åˆ°åç§°
                model_names = {
                    0: "SIMPLE_PINHOLE",
                    1: "PINHOLE", 
                    2: "SIMPLE_RADIAL",
                    3: "RADIAL",
                    4: "OPENCV",
                    5: "OPENCV_FISHEYE",
                    6: "FULL_OPENCV",
                    7: "FOV",
                    8: "SIMPLE_RADIAL_FISHEYE",
                    9: "RADIAL_FISHEYE",
                    10: "THIN_PRISM_FISHEYE"
                }
                camera_model = model_names.get(camera.model_id, f"MODEL_{camera.model_id}")
            
            # å¦‚æœç„¦è·ä¸ºNoneï¼Œå°è¯•ä»å…¶ä»–é€”å¾„è·å–
            if focal_length is None:
                if hasattr(camera, 'focal_length'):
                    focal_length = float(camera.focal_length)
                    focal_length_y = focal_length
                elif len(principal_point) > 0 and image_size[0] > 0:
                    # ä¼°ç®—ç„¦è·ï¼ˆå‡è®¾FOVçº¦ä¸º60åº¦ï¼‰
                    focal_length = image_size[0] * 0.5 / np.tan(np.radians(30))
                    focal_length_y = focal_length
                else:
                    focal_length = 800.0  # é»˜è®¤å€¼
                    focal_length_y = 800.0
            
            intrinsics = {
                "focal_length": focal_length,
                "focal_length_y": focal_length_y,
                "principal_point": principal_point,
                "image_size": image_size,
                "camera_model": camera_model,
                "distortion": distortion
            }
            
            print(f"è§£æç›¸æœºå†…å‚: ç„¦è·={focal_length:.2f}, ä¸»ç‚¹={principal_point}, å°ºå¯¸={image_size}, æ¨¡å‹={camera_model}")
            
        except Exception as e:
            print(f"è§£æç›¸æœºå†…å‚æ—¶å‡ºé”™: {e}")
            # æä¾›é»˜è®¤çš„å†…å‚
            intrinsics = {
                "focal_length": 800.0,
                "focal_length_y": 800.0,
                "principal_point": [320.0, 240.0],
                "image_size": [640, 480],
                "camera_model": "PINHOLE",
                "distortion": []
            }
        
        # è·å–ç›¸æœºä½å§¿
        poses = []
        try:
            print(f"å¼€å§‹è§£æ {len(reconstruction.images)} å¼ å›¾åƒçš„ä½å§¿...")
            
            # å…ˆæ£€æŸ¥ç¬¬ä¸€ä¸ªå›¾åƒçš„å±æ€§æ¥äº†è§£æ•°æ®ç»“æ„
            if len(reconstruction.images) > 0:
                first_image = list(reconstruction.images.values())[0]
                print(f"ç¬¬ä¸€ä¸ªå›¾åƒçš„å±æ€§: {[attr for attr in dir(first_image) if not attr.startswith('_')]}")
                
                # æ£€æŸ¥ä½å§¿ç›¸å…³å±æ€§
                if hasattr(first_image, 'qvec'):
                    print(f"qvec ç±»å‹: {type(first_image.qvec)}, å€¼: {first_image.qvec}")
                if hasattr(first_image, 'tvec'):
                    print(f"tvec ç±»å‹: {type(first_image.tvec)}, å€¼: {first_image.tvec}")
                if hasattr(first_image, 'camera_to_world'):
                    print(f"camera_to_world å­˜åœ¨")
                if hasattr(first_image, 'world_to_camera'):
                    print(f"world_to_camera å­˜åœ¨")
            
            for idx, (image_id, image) in enumerate(reconstruction.images.items()):
                print(f"å¤„ç†å›¾åƒ {idx+1}: ID={image_id}")
                
                # æ£€æŸ¥å›¾åƒæ˜¯å¦å·²æ³¨å†Œ
                is_registered = True
                if hasattr(image, 'registered'):
                    is_registered = image.registered
                    print(f"  å›¾åƒregisteredçŠ¶æ€: {is_registered}")
                
                # è·å–ä½å§¿æ•°æ®
                quat = None
                trans = None
                image_name = "unknown"
                
                # æ–¹æ³•1: ç›´æ¥è·å–qvecå’Œtvec (COLMAPæ ¼å¼)
                if hasattr(image, 'qvec') and hasattr(image, 'tvec'):
                    try:
                        # PyColmapçš„qvecå’Œtvecåº”è¯¥æ˜¯numpyæ•°ç»„
                        qvec_raw = image.qvec
                        tvec_raw = image.tvec
                        
                        print(f"  åŸå§‹qvecç±»å‹: {type(qvec_raw)}, å½¢çŠ¶: {getattr(qvec_raw, 'shape', 'no shape')}")
                        print(f"  åŸå§‹tvecç±»å‹: {type(tvec_raw)}, å½¢çŠ¶: {getattr(tvec_raw, 'shape', 'no shape')}")
                        print(f"  qvecå€¼: {qvec_raw}")
                        print(f"  tvecå€¼: {tvec_raw}")
                        
                        # è½¬æ¢ä¸ºåˆ—è¡¨
                        if hasattr(qvec_raw, 'tolist'):
                            quat = qvec_raw.tolist()
                        else:
                            quat = list(qvec_raw)
                            
                        if hasattr(tvec_raw, 'tolist'):
                            trans = tvec_raw.tolist()
                        else:
                            trans = list(tvec_raw)
                        
                        print(f"  è½¬æ¢åquat: {quat}")
                        print(f"  è½¬æ¢åtrans: {trans}")
                        
                        # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
                        if len(quat) == 4 and len(trans) == 3:
                            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å››å…ƒæ•°ï¼ˆéé›¶ï¼‰
                            quat_magnitude = sum(x*x for x in quat) ** 0.5
                            if quat_magnitude > 1e-6:
                                print(f"  ä½å§¿æ•°æ®æœ‰æ•ˆ: quat_magnitude={quat_magnitude}")
                            else:
                                print(f"  å››å…ƒæ•°å¹…åº¦å¤ªå°ï¼Œå¯èƒ½æ— æ•ˆ: {quat_magnitude}")
                                quat = None
                                trans = None
                        else:
                            print(f"  ä½å§¿æ•°æ®æ ¼å¼é”™è¯¯: quaté•¿åº¦={len(quat)}, transé•¿åº¦={len(trans)}")
                            quat = None
                            trans = None
                            
                    except Exception as e:
                        print(f"  æå–qvec/tvecå¤±è´¥: {e}")
                        quat = None
                        trans = None
                
                # æ–¹æ³•2: å°è¯•è·å–å˜æ¢çŸ©é˜µ
                elif hasattr(image, 'camera_to_world') or hasattr(image, 'world_to_camera'):
                    try:
                        print(f"  å°è¯•ä»å˜æ¢çŸ©é˜µè·å–ä½å§¿")
                        
                        if hasattr(image, 'camera_to_world'):
                            matrix = image.camera_to_world()
                        elif hasattr(image, 'world_to_camera'):
                            matrix = image.world_to_camera()
                            # éœ€è¦æ±‚é€†å¾—åˆ°camera_to_world
                            matrix = np.linalg.inv(matrix)
                        
                        print(f"  å˜æ¢çŸ©é˜µ: {matrix}")
                        
                        # ä»4x4å˜æ¢çŸ©é˜µæå–æ—‹è½¬å’Œå¹³ç§»
                        rotation_matrix = matrix[:3, :3]
                        translation = matrix[:3, 3]
                        
                        # æ—‹è½¬çŸ©é˜µè½¬å››å…ƒæ•°
                        from scipy.spatial.transform import Rotation
                        r = Rotation.from_matrix(rotation_matrix)
                        quat_xyzw = r.as_quat()  # [x, y, z, w]
                        quat = [quat_xyzw[3], quat_xyzw[0], quat_xyzw[1], quat_xyzw[2]]  # [w, x, y, z]
                        trans = translation.tolist()
                        
                        print(f"  ä»çŸ©é˜µæå–: quat={quat}, trans={trans}")
                        
                    except Exception as e:
                        print(f"  ä»å˜æ¢çŸ©é˜µæå–å¤±è´¥: {e}")
                        quat = None
                        trans = None
                
                # è·å–å›¾åƒåç§°
                if hasattr(image, 'name'):
                    image_name = image.name
                elif hasattr(image, 'filename'):
                    image_name = image.filename
                else:
                    image_name = f"image_{image_id}"
                
                # å¦‚æœä»ç„¶æ²¡æœ‰æœ‰æ•ˆä½å§¿ï¼Œä½†å›¾åƒå·²æ³¨å†Œï¼Œå°è¯•å…¶ä»–æ–¹æ³•
                if (not quat or not trans) and is_registered:
                    print(f"  å›¾åƒå·²æ³¨å†Œä½†æ— æ³•è·å–ä½å§¿ï¼Œå°è¯•å…¶ä»–å±æ€§...")
                    
                    # åˆ—å‡ºæ‰€æœ‰æ•°å€¼å±æ€§
                    numeric_attrs = []
                    for attr_name in dir(image):
                        if not attr_name.startswith('_'):
                            try:
                                attr_value = getattr(image, attr_name)
                                if hasattr(attr_value, 'shape') or isinstance(attr_value, (list, tuple)):
                                    numeric_attrs.append((attr_name, type(attr_value), getattr(attr_value, 'shape', len(attr_value) if hasattr(attr_value, '__len__') else 'no len')))
                            except:
                                pass
                    print(f"  æ•°å€¼å±æ€§: {numeric_attrs}")
                
                # å¦‚æœä»ç„¶æ²¡æœ‰ä½å§¿æ•°æ®ï¼Œåˆ›å»ºåŸºäºå›¾åƒé¡ºåºçš„ä¼°è®¡ä½å§¿
                if not quat or not trans:
                    if is_registered:
                        print(f"  ä¸ºå·²æ³¨å†Œå›¾åƒåˆ›å»ºä¼°è®¡ä½å§¿")
                        # åˆ›å»ºåœ†å¼§è½¨è¿¹è€Œä¸æ˜¯ç›´çº¿
                        angle = idx * 2 * np.pi / max(len(reconstruction.images), 1)
                        radius = 2.0
                        quat = [1, 0, 0, 0]  # æ— æ—‹è½¬
                        trans = [radius * np.cos(angle), radius * np.sin(angle), idx * 0.1]
                    else:
                        print(f"  å›¾åƒæœªæ³¨å†Œï¼Œè·³è¿‡")
                        continue
                
                # è½¬æ¢ä¸ºæ¬§æ‹‰è§’
                try:
                    euler = self._quaternion_to_euler(quat)
                except Exception as e:
                    print(f"  å››å…ƒæ•°è½¬æ¬§æ‹‰è§’å¤±è´¥: {e}")
                    euler = [0, 0, 0]
                
                pose = {
                    "position": trans,
                    "rotation_quaternion": quat,
                    "rotation_euler": euler,
                    "image_name": image_name,
                    "is_registered": is_registered,
                    "image_id": image_id
                }
                poses.append(pose)
                print(f"  æ·»åŠ ä½å§¿: position={trans}, euler={[f'{x:.3f}' for x in euler]}")
                        
        except Exception as e:
            print(f"è§£æç›¸æœºä½å§¿æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            
            # å¦‚æœæ— æ³•è§£æä½å§¿ï¼Œåˆ›å»ºåœ†å¼§è™šæ‹Ÿä½å§¿
            print("åˆ›å»ºåœ†å¼§è™šæ‹Ÿä½å§¿ä½œä¸ºå¤‡ç”¨...")
            poses = []
            num_images = len(reconstruction.images) if hasattr(reconstruction, 'images') else len(frame_paths)
            for i in range(min(num_images, len(frame_paths))):
                angle = i * 2 * np.pi / max(num_images, 1)
                radius = 3.0
                pose = {
                    "position": [radius * np.cos(angle), radius * np.sin(angle), i * 0.2],
                    "rotation_quaternion": [np.cos(angle/2), 0, 0, np.sin(angle/2)],  # ç»•Zè½´æ—‹è½¬
                    "rotation_euler": [0, 0, angle],
                    "image_name": os.path.basename(frame_paths[i]) if i < len(frame_paths) else f"frame_{i}",
                    "is_registered": False,
                    "image_id": i
                }
                poses.append(pose)
        
        print(f"æœ€ç»ˆè§£æä½å§¿æ•°é‡: {len(poses)}")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        statistics = self._calculate_trajectory_statistics(poses)
        
        # ç‚¹äº‘ä¿¡æ¯
        point_cloud_info = {
            "num_points": len(reconstruction.points3D) if hasattr(reconstruction, 'points3D') else 0,
            "dense_reconstruction": dense_info
        }
        
        print(f"è§£æå®Œæˆ: {len(poses)} ä¸ªä½å§¿, {point_cloud_info['num_points']} ä¸ª3Dç‚¹")
        
        return {
            "success": True,
            "intrinsics": intrinsics,
            "poses": poses,
            "statistics": statistics,
            "frame_count": len(poses),
            "trajectory_visualization": None,  # å°†åœ¨ä¸»å‡½æ•°ä¸­ç”Ÿæˆ
            "point_cloud": point_cloud_info
        }

    def _parse_colmap_text_files(self, reconstruction_dir: str, frame_paths: List[str], dense_info: Dict) -> Dict:
        """è§£æCOLMAPæ–‡æœ¬æ ¼å¼ç»“æœæ–‡ä»¶"""
        
        cameras_file = os.path.join(reconstruction_dir, "cameras.txt")
        images_file = os.path.join(reconstruction_dir, "images.txt")
        points_file = os.path.join(reconstruction_dir, "points3D.txt")
        
        if not all(os.path.exists(f) for f in [cameras_file, images_file]):
            raise FileNotFoundError("COLMAPç»“æœæ–‡ä»¶ä¸å®Œæ•´")
        
        # è§£æç›¸æœºå‚æ•°
        intrinsics = None
        with open(cameras_file, 'r') as f:
            for line in f:
                if line.startswith('#'):
                    continue
                parts = line.strip().split()
                if len(parts) >= 5:
                    camera_id = int(parts[0])
                    model = parts[1]
                    width = int(parts[2])
                    height = int(parts[3])
                    params = [float(x) for x in parts[4:]]
                    
                    intrinsics = {
                        "focal_length": params[0],
                        "focal_length_y": params[1] if len(params) > 1 else params[0],
                        "principal_point": [params[2], params[3]] if len(params) > 3 else [width/2, height/2],
                        "image_size": [width, height],
                        "camera_model": model,
                        "distortion": params[4:] if len(params) > 4 else []
                    }
                    break
        
        if intrinsics is None:
            raise ValueError("æ— æ³•è§£æç›¸æœºå†…å‚")
        
        # è§£æå›¾åƒä½å§¿
        poses = []
        with open(images_file, 'r') as f:
            for line in f:
                if line.startswith('#'):
                    continue
                parts = line.strip().split()
                if len(parts) >= 10:
                    image_id = int(parts[0])
                    qw, qx, qy, qz = float(parts[1]), float(parts[2]), float(parts[3]), float(parts[4])
                    tx, ty, tz = float(parts[5]), float(parts[6]), float(parts[7])
                    camera_id = int(parts[8])
                    image_name = parts[9]
                    
                    quat = [qw, qx, qy, qz]
                    trans = [tx, ty, tz]
                    euler = self._quaternion_to_euler(quat)
                    
                    pose = {
                        "position": trans,
                        "rotation_quaternion": quat,
                        "rotation_euler": euler,
                        "image_name": image_name
                    }
                    poses.append(pose)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        statistics = self._calculate_trajectory_statistics(poses)
        
        # è®¡ç®—ç‚¹äº‘æ•°é‡
        num_points = 0
        if os.path.exists(points_file):
            with open(points_file, 'r') as f:
                for line in f:
                    if not line.startswith('#') and line.strip():
                        num_points += 1
        
        point_cloud_info = {
            "num_points": num_points,
            "dense_reconstruction": dense_info
        }
        
        return {
            "success": True,
            "intrinsics": intrinsics,
            "poses": poses,
            "statistics": statistics,
            "frame_count": len(poses),
            "trajectory_visualization": None,
            "point_cloud": point_cloud_info
        }

    def _quaternion_to_euler(self, quat: List[float]) -> List[float]:
        """å››å…ƒæ•°è½¬æ¬§æ‹‰è§’"""
        qw, qx, qy, qz = quat
        
        # Roll (x-axis rotation)
        sinr_cosp = 2 * (qw * qx + qy * qz)
        cosr_cosp = 1 - 2 * (qx * qx + qy * qy)
        roll = np.arctan2(sinr_cosp, cosr_cosp)
        
        # Pitch (y-axis rotation)
        sinp = 2 * (qw * qy - qz * qx)
        if abs(sinp) >= 1:
            pitch = np.copysign(np.pi / 2, sinp)
        else:
            pitch = np.arcsin(sinp)
        
        # Yaw (z-axis rotation)
        siny_cosp = 2 * (qw * qz + qx * qy)
        cosy_cosp = 1 - 2 * (qy * qy + qz * qz)
        yaw = np.arctan2(siny_cosp, cosy_cosp)
        
        return [roll, pitch, yaw]

    def _calculate_trajectory_statistics(self, poses: List[Dict]) -> Dict:
        """è®¡ç®—è½¨è¿¹ç»Ÿè®¡ä¿¡æ¯"""
        if len(poses) < 2:
            return {"total_distance": 0, "average_speed": 0, "num_poses": len(poses)}
        
        total_distance = 0
        for i in range(1, len(poses)):
            pos1 = np.array(poses[i-1]["position"])
            pos2 = np.array(poses[i]["position"])
            distance = np.linalg.norm(pos2 - pos1)
            total_distance += distance
        
        average_speed = total_distance / (len(poses) - 1)
        
        return {
            "total_distance": float(total_distance),
            "average_speed": float(average_speed),
            "num_poses": len(poses)
        }

    def _create_colmap_visualization(self, poses: List[Dict]) -> np.ndarray:
        """åˆ›å»ºCOLMAPè½¨è¿¹å¯è§†åŒ–"""
        print(f"åˆ›å»ºè½¨è¿¹å¯è§†åŒ–ï¼Œä½å§¿æ•°é‡: {len(poses)}")
        
        if len(poses) == 0:
            return self._create_empty_visualization_with_message("æ²¡æœ‰ä½å§¿æ•°æ®").squeeze(0).numpy()
        
        # æå–ä½ç½®ä¿¡æ¯
        positions = np.array([pose["position"] for pose in poses])
        print(f"ä½ç½®æ•°æ®å½¢çŠ¶: {positions.shape}")
        print(f"ä½ç½®èŒƒå›´: X=[{positions[:, 0].min():.3f}, {positions[:, 0].max():.3f}], Y=[{positions[:, 1].min():.3f}, {positions[:, 1].max():.3f}], Z=[{positions[:, 2].min():.3f}, {positions[:, 2].max():.3f}]")
        
        # åˆ›å»º3Dè½¨è¿¹çš„å¤šè§†å›¾å¯è§†åŒ–
        img_size = (1200, 800)
        img = np.ones((img_size[1], img_size[0], 3), dtype=np.uint8) * 255
        
        # è®¡ç®—ä½ç½®çš„è¾¹ç•Œ
        pos_min = positions.min(axis=0)
        pos_max = positions.max(axis=0)
        pos_range = pos_max - pos_min
        pos_center = (pos_min + pos_max) / 2
        
        print(f"ä½ç½®ç»Ÿè®¡: min={pos_min}, max={pos_max}, range={pos_range}, center={pos_center}")
        
        # ç»˜åˆ¶ä¸‰ä¸ªè§†å›¾ï¼šXY (ä¿¯è§†å›¾), XZ (ä¾§è§†å›¾), YZ (æ­£è§†å›¾)
        views = [
            {"name": "Top View (XY)", "indices": [0, 1], "pos": (50, 50), "size": (350, 250)},
            {"name": "Side View (XZ)", "indices": [0, 2], "pos": (450, 50), "size": (350, 250)},
            {"name": "Front View (YZ)", "indices": [1, 2], "pos": (850, 50), "size": (300, 250)}
        ]
        
        colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]  # çº¢ç»¿è“
        
        for view_idx, view in enumerate(views):
            # è·å–è§†å›¾åŒºåŸŸ
            x_start, y_start = view["pos"]
            view_width, view_height = view["size"]
            
            # ç»˜åˆ¶è§†å›¾è¾¹æ¡†
            cv2.rectangle(img, (x_start-5, y_start-5), 
                         (x_start+view_width+5, y_start+view_height+5), (100, 100, 100), 2)
            
            # æ·»åŠ è§†å›¾æ ‡é¢˜
            cv2.putText(img, view["name"], (x_start, y_start-10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)
            
            # æå–è¯¥è§†å›¾çš„2Dåæ ‡
            coord_indices = view["indices"]
            pos_2d = positions[:, coord_indices]
            
            if pos_range[coord_indices].max() > 1e-6:
                # å½’ä¸€åŒ–åˆ°è§†å›¾èŒƒå›´
                margin = 20
                if pos_range[coord_indices].max() > 0:
                    pos_norm = (pos_2d - pos_min[coord_indices]) / pos_range[coord_indices].max()
                else:
                    pos_norm = np.zeros_like(pos_2d)
                
                pos_img = pos_norm * (min(view_width, view_height) - 2 * margin) + margin
                pos_img[:, 0] += x_start
                pos_img[:, 1] += y_start
                pos_img = pos_img.astype(int)
                
                # ç»˜åˆ¶è½¨è¿¹çº¿
                for i in range(1, len(pos_img)):
                    cv2.line(img, tuple(pos_img[i-1]), tuple(pos_img[i]), colors[view_idx], 2)
                
                # ç»˜åˆ¶ä½å§¿ç‚¹
                for i, pos in enumerate(pos_img):
                    # èµ·ç‚¹ç»¿è‰²ï¼Œç»ˆç‚¹çº¢è‰²ï¼Œä¸­é—´ç‚¹è“è‰²
                    if i == 0:
                        color = (0, 255, 0)  # ç»¿è‰²èµ·ç‚¹
                        radius = 8
                    elif i == len(pos_img) - 1:
                        color = (0, 0, 255)  # çº¢è‰²ç»ˆç‚¹
                        radius = 8
                    else:
                        color = (255, 100, 0)  # æ©™è‰²ä¸­é—´ç‚¹
                        radius = 4
                    
                    cv2.circle(img, tuple(pos), radius, color, -1)
                    
                    # æ¯5ä¸ªç‚¹æ ‡è®°ä¸€ä¸ªç¼–å·
                    if i % 5 == 0 or i == len(pos_img) - 1:
                        cv2.putText(img, str(i), (pos[0]+10, pos[1]), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
            else:
                # ä½ç½®å˜åŒ–å¾ˆå°ï¼Œæ˜¾ç¤ºä¸­å¿ƒç‚¹
                center_x = x_start + view_width // 2
                center_y = y_start + view_height // 2
                cv2.circle(img, (center_x, center_y), 10, colors[view_idx], -1)
                cv2.putText(img, "Static", (center_x-20, center_y+25), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, colors[view_idx], 2)
        
        # æ·»åŠ 3Dä¿¡æ¯é¢æ¿
        info_y_start = 350
        cv2.putText(img, "COLMAP 3D Camera Trajectory", (50, info_y_start), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 0), 2)
        
        info_lines = [
            f"Total Poses: {len(poses)}",
            f"Position Range:",
            f"  X: [{pos_min[0]:.3f}, {pos_max[0]:.3f}] (range: {pos_range[0]:.3f})",
            f"  Y: [{pos_min[1]:.3f}, {pos_max[1]:.3f}] (range: {pos_range[1]:.3f})",
            f"  Z: [{pos_min[2]:.3f}, {pos_max[2]:.3f}] (range: {pos_range[2]:.3f})",
            f"Center: [{pos_center[0]:.3f}, {pos_center[1]:.3f}, {pos_center[2]:.3f}]"
        ]
        
        # æ·»åŠ æ³¨å†ŒçŠ¶æ€ä¿¡æ¯
        registered_count = sum(1 for pose in poses if pose.get("is_registered", True))
        info_lines.append(f"Registered: {registered_count}/{len(poses)}")
        
        for i, line in enumerate(info_lines):
            cv2.putText(img, line, (50, info_y_start + 40 + i * 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)
        
        # æ·»åŠ å›¾ä¾‹
        legend_x = 50
        legend_y = info_y_start + 40 + len(info_lines) * 30 + 20
        cv2.putText(img, "Legend:", (legend_x, legend_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)
        
        legend_items = [
            ("Start", (0, 255, 0)),
            ("End", (0, 0, 255)), 
            ("Path", (255, 100, 0))
        ]
        
        for i, (label, color) in enumerate(legend_items):
            y_pos = legend_y + 25 + i * 25
            cv2.circle(img, (legend_x + 10, y_pos), 6, color, -1)
            cv2.putText(img, label, (legend_x + 25, y_pos + 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
        
        return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    def _create_empty_visualization_with_message(self, message: str) -> torch.Tensor:
        """åˆ›å»ºå¸¦æ¶ˆæ¯çš„ç©ºå¯è§†åŒ–å›¾åƒ"""
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„å ä½å›¾åƒ
        img = np.ones((400, 600, 3), dtype=np.float32) * 0.1  # æ·±ç°è‰²èƒŒæ™¯
        
        # æ·»åŠ æ¶ˆæ¯
        try:
            img_uint8 = (img * 255).astype(np.uint8)
            
            # åˆ†è¡Œæ˜¾ç¤ºæ¶ˆæ¯
            lines = message.split('\n') if '\n' in message else [message]
            y_start = 180
            for i, line in enumerate(lines):
                y_pos = y_start + i * 30
                cv2.putText(img_uint8, line, (50, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
            
            img = img_uint8.astype(np.float32) / 255.0
        except:
            pass  # å¦‚æœæ·»åŠ æ–‡å­—å¤±è´¥ï¼Œè¿”å›çº¯è‰²å›¾åƒ
        
        return torch.from_numpy(img).unsqueeze(0)

    def _format_point_cloud_output(self, point_cloud_info: Dict, output_format: str) -> str:
        """æ ¼å¼åŒ–ç‚¹äº‘ä¿¡æ¯è¾“å‡º"""
        if not point_cloud_info:
            return json.dumps({"point_cloud": "æœªç”Ÿæˆ"}, ensure_ascii=False, indent=2)
        
        if output_format == "json":
            # ç®€åŒ–ç‰ˆæœ¬
            simplified = {
                "num_points": point_cloud_info.get("num_points", 0),
                "has_dense": "dense_reconstruction" in point_cloud_info
            }
            return json.dumps(simplified, ensure_ascii=False, indent=2)
        else:
            # è¯¦ç»†ç‰ˆæœ¬
            return json.dumps(point_cloud_info, ensure_ascii=False, indent=2)

    def _is_valid_video_format(self, video_path: str) -> bool:
        """éªŒè¯è§†é¢‘æ ¼å¼"""
        valid_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv', '.webm']
        ext = os.path.splitext(video_path)[1].lower()
        return ext in valid_extensions

    def _format_intrinsics_output(self, intrinsics: Dict, output_format: str) -> str:
        """æ ¼å¼åŒ–å†…å‚è¾“å‡º"""
        if output_format == "json":
            # ç®€åŒ–ç‰ˆæœ¬ï¼ŒåªåŒ…å«æ ¸å¿ƒå‚æ•°
            simplified = {
                "focal_length": intrinsics["focal_length"],
                "principal_point": intrinsics["principal_point"],
                "image_size": intrinsics["image_size"]
            }
            return json.dumps(simplified, ensure_ascii=False, indent=2)
        else:
            # è¯¦ç»†ç‰ˆæœ¬
            return json.dumps(intrinsics, ensure_ascii=False, indent=2)

    def _format_poses_output(self, poses: List[Dict], output_format: str) -> str:
        """æ ¼å¼åŒ–ä½å§¿è¾“å‡º"""
        if output_format == "json":
            # ç®€åŒ–ç‰ˆæœ¬ï¼ŒåªåŒ…å«ä½ç½®å’Œæ¬§æ‹‰è§’
            simplified_poses = []
            for i, pose in enumerate(poses):
                simplified_poses.append({
                    "frame": i,
                    "position": pose["position"],
                    "rotation_euler": pose["rotation_euler"]
                })
            return json.dumps(simplified_poses, ensure_ascii=False, indent=2)
        else:
            # è¯¦ç»†ç‰ˆæœ¬
            detailed_poses = []
            for i, pose in enumerate(poses):
                detailed_pose = pose.copy()
                detailed_pose["frame"] = i
                detailed_poses.append(detailed_pose)
            return json.dumps(detailed_poses, ensure_ascii=False, indent=2)

    def _format_statistics_output(self, statistics: Dict, result: Dict, output_format: str) -> str:
        """æ ¼å¼åŒ–ç»Ÿè®¡ä¿¡æ¯è¾“å‡º"""
        if output_format == "json":
            # ç®€åŒ–ç‰ˆæœ¬
            simplified_stats = {
                "frame_count": result["frame_count"],
                "total_distance": statistics["total_distance"],
                "success": result["success"]
            }
            return json.dumps(simplified_stats, ensure_ascii=False, indent=2)
        else:
            # è¯¦ç»†ç‰ˆæœ¬
            detailed_stats = statistics.copy()
            detailed_stats.update({
                "frame_count": result["frame_count"],
                "success": result["success"],
                "processing_info": {
                    "total_poses": len(result["poses"]),
                    "has_visualization": result.get("trajectory_visualization") is not None
                }
            })
            return json.dumps(detailed_stats, ensure_ascii=False, indent=2)

    def _prepare_visualization_image(self, trajectory_data) -> torch.Tensor:
        """å‡†å¤‡å¯è§†åŒ–å›¾åƒ"""
        try:
            if isinstance(trajectory_data, list):
                # ä»åˆ—è¡¨è½¬æ¢ä¸ºnumpyæ•°ç»„
                img_array = np.array(trajectory_data, dtype=np.uint8)
            else:
                img_array = trajectory_data
            
            if len(img_array.shape) == 3 and img_array.shape[2] == 3:
                # BGRè½¬RGB
                img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)
                # è½¬æ¢ä¸ºtorch tensor (H, W, C) -> (1, H, W, C)
                img_tensor = torch.from_numpy(img_array.astype(np.float32) / 255.0)
                return img_tensor.unsqueeze(0)
            else:
                return self._create_empty_visualization()
                
        except Exception as e:
            print(f"å¯è§†åŒ–å›¾åƒå¤„ç†é”™è¯¯: {e}")
            return self._create_empty_visualization()

    def _create_empty_visualization(self) -> torch.Tensor:
        """åˆ›å»ºç©ºçš„å¯è§†åŒ–å›¾åƒ"""
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„å ä½å›¾åƒ
        img = np.ones((400, 400, 3), dtype=np.float32) * 0.1  # æ·±ç°è‰²èƒŒæ™¯
        
        # æ·»åŠ æ–‡å­—æç¤º
        try:
            img_uint8 = (img * 255).astype(np.uint8)
            cv2.putText(img_uint8, "No Trajectory", (120, 180), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 2)
            cv2.putText(img_uint8, "Visualization", (110, 220), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 2)
            img = img_uint8.astype(np.float32) / 255.0
        except:
            pass  # å¦‚æœæ·»åŠ æ–‡å­—å¤±è´¥ï¼Œè¿”å›çº¯è‰²å›¾åƒ
        
        return torch.from_numpy(img).unsqueeze(0)

    def _estimate_with_colmap_cli(self, video_path: str, frame_interval: int, max_frames: int,
                                feature_type: str, matcher_type: str, quality: str, enable_dense: bool) -> Dict:
        """ä½¿ç”¨å‘½ä»¤è¡ŒCOLMAPè¿›è¡Œç›¸æœºå‚æ•°ä¼°è®¡ï¼ˆåŸæœ‰å®ç°ï¼‰"""
        
        # åˆ›å»ºä¸´æ—¶å·¥ä½œç›®å½•
        temp_dir = tempfile.mkdtemp(prefix="colmap_estimation_")
        images_dir = os.path.join(temp_dir, "images")
        database_path = os.path.join(temp_dir, "database.db")
        sparse_dir = os.path.join(temp_dir, "sparse")
        dense_dir = os.path.join(temp_dir, "dense")
        
        try:
            os.makedirs(images_dir, exist_ok=True)
            os.makedirs(sparse_dir, exist_ok=True)
            if enable_dense:
                os.makedirs(dense_dir, exist_ok=True)
            
            # 1. æå–è§†é¢‘å¸§
            print("æå–è§†é¢‘å¸§...")
            frame_paths = self._extract_frames_for_colmap(video_path, images_dir, frame_interval, max_frames)
            
            if len(frame_paths) < 3:
                raise ValueError("æå–çš„å¸§æ•°è¿‡å°‘ï¼ˆ< 3ï¼‰ï¼Œæ— æ³•è¿›è¡Œé‡å»º")
            
            # 2. ç‰¹å¾æå–
            print(f"ä½¿ç”¨ {feature_type} è¿›è¡Œç‰¹å¾æå–...")
            self._run_colmap_feature_extraction(database_path, images_dir, feature_type, quality)
            
            # 3. ç‰¹å¾åŒ¹é…
            print(f"ä½¿ç”¨ {matcher_type} è¿›è¡Œç‰¹å¾åŒ¹é…...")
            self._run_colmap_matching(database_path, matcher_type)
            
            # 4. ç¨€ç–é‡å»º
            print("è¿›è¡Œç¨€ç–é‡å»º...")
            reconstruction_dir = os.path.join(sparse_dir, "0")
            os.makedirs(reconstruction_dir, exist_ok=True)
            self._run_colmap_sparse_reconstruction(database_path, images_dir, reconstruction_dir)
            
            # 5. å¯†é›†é‡å»ºï¼ˆå¯é€‰ï¼‰
            dense_info = {}
            if enable_dense:
                print("è¿›è¡Œå¯†é›†é‡å»º...")
                dense_info = self._run_colmap_dense_reconstruction(images_dir, reconstruction_dir, dense_dir)
            
            # 6. è§£æç»“æœ
            print("è§£æé‡å»ºç»“æœ...")
            result = self._parse_colmap_results(reconstruction_dir, frame_paths, dense_info)
            
            # 7. ç”Ÿæˆå¯è§†åŒ–
            if result["success"]:
                result["trajectory_visualization"] = self._create_colmap_visualization(result["poses"])
            
            return result
            
        except Exception as e:
            print(f"COLMAP ä¼°è®¡è¿‡ç¨‹å‡ºé”™: {e}")
            return {
                "success": False,
                "error": str(e),
                "intrinsics": None,
                "poses": [],
                "statistics": {"total_distance": 0, "average_speed": 0},
                "frame_count": 0,
                "trajectory_visualization": None,
                "point_cloud": {}
            }
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                shutil.rmtree(temp_dir)
            except:
                pass

    def _extract_frames_for_colmap(self, video_path: str, output_dir: str, interval: int, max_frames: int) -> List[str]:
        """ä¸ºCOLMAPæå–è§†é¢‘å¸§"""
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        frame_paths = []
        frame_count = 0
        current_frame = 0
        
        while current_frame < total_frames and frame_count < max_frames:
            cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame)
            ret, frame = cap.read()
            
            if ret:
                # ä¿å­˜å¸§
                frame_filename = f"frame_{frame_count:06d}.jpg"
                frame_path = os.path.join(output_dir, frame_filename)
                cv2.imwrite(frame_path, frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
                frame_paths.append(frame_path)
                frame_count += 1
            
            current_frame += interval
        
        cap.release()
        return frame_paths

# ç¬¬äºŒä¸ªèŠ‚ç‚¹ï¼šè§†é¢‘å¸§æå–å™¨ï¼ˆè¾…åŠ©èŠ‚ç‚¹ï¼‰
class VideoFrameExtractor:
    """è§†é¢‘å¸§æå–èŠ‚ç‚¹ï¼ˆè¾…åŠ©åŠŸèƒ½ï¼‰"""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "video_path": ("STRING", {
                    "default": "",
                    "tooltip": "è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„"
                }),
                "frame_indices": ("STRING", {
                    "default": "0,10,20,30",
                    "tooltip": "è¦æå–çš„å¸§ç´¢å¼•ï¼Œç”¨é€—å·åˆ†éš”"
                })
            },
            "optional": {
                "resize_width": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 2048,
                    "tooltip": "è°ƒæ•´å›¾åƒå®½åº¦ï¼Œ0è¡¨ç¤ºä¿æŒåŸå°ºå¯¸"
                }),
                "resize_height": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 2048,
                    "tooltip": "è°ƒæ•´å›¾åƒé«˜åº¦ï¼Œ0è¡¨ç¤ºä¿æŒåŸå°ºå¯¸"
                })
            }
        }

    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("extracted_frames", "frame_info")
    OUTPUT_IS_LIST = (True, False)
    FUNCTION = "extract_frames"
    CATEGORY = "ğŸ’ƒVVL/VideoCamera"

    def extract_frames(self, video_path: str, frame_indices: str, resize_width: int = 0, resize_height: int = 0) -> tuple:
        """æå–æŒ‡å®šçš„è§†é¢‘å¸§"""
        try:
            if not os.path.exists(video_path):
                raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            
            # è§£æå¸§ç´¢å¼•
            indices = [int(x.strip()) for x in frame_indices.split(',') if x.strip()]
            
            cap = cv2.VideoCapture(video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            
            extracted_frames = []
            extracted_info = []
            
            for idx in indices:
                if 0 <= idx < total_frames:
                    cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                    ret, frame = cap.read()
                    
                    if ret:
                        # BGRè½¬RGB
                        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                        
                        # è°ƒæ•´å°ºå¯¸
                        if resize_width > 0 and resize_height > 0:
                            frame = cv2.resize(frame, (resize_width, resize_height))
                        
                        # è½¬æ¢ä¸ºtorch tensor
                        frame_tensor = torch.from_numpy(frame.astype(np.float32) / 255.0)
                        extracted_frames.append(frame_tensor)
                        
                        extracted_info.append({
                            "frame_index": idx,
                            "shape": list(frame.shape)
                        })
            
            cap.release()
            
            info_json = json.dumps({
                "total_extracted": len(extracted_frames),
                "video_total_frames": total_frames,
                "frame_details": extracted_info
            }, ensure_ascii=False, indent=2)
            
            return (extracted_frames, info_json)
            
        except Exception as e:
            error_msg = f"å¸§æå–é”™è¯¯: {str(e)}"
            print(error_msg)
            
            # è¿”å›ç©ºå›¾åƒå’Œé”™è¯¯ä¿¡æ¯
            empty_frame = torch.zeros((400, 400, 3), dtype=torch.float32)
            error_json = json.dumps({"error": error_msg}, ensure_ascii=False, indent=2)
            
            return ([empty_frame], error_json)

# èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "VideoCameraEstimator": VideoCameraEstimator,
    "VideoFrameExtractor": VideoFrameExtractor
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "VideoCameraEstimator": "VVL Video Camera Estimator",
    "VideoFrameExtractor": "VVL Video Frame Extractor"
} 