import torch
import numpy as np
import json
import os
import tempfile
import cv2
import shutil
from PIL import Image
from typing import List, Dict, Any
import pathlib

# æ·»åŠ ComfyUIç±»å‹å¯¼å…¥
try:
    from comfy.comfy_types import IO
except ImportError:
    # å¦‚æœæ— æ³•å¯¼å…¥ï¼Œåˆ›å»ºä¸€ä¸ªå…¼å®¹çš„ç±»
    class IO:
        IMAGE = "IMAGE"

class ColmapCameraEstimator:
    """ä½¿ç”¨COLMAPè¿›è¡Œç›¸æœºå‚æ•°ä¼°è®¡çš„æ ¸å¿ƒç±»"""

    def __init__(self):
        self.check_dependencies()

    def check_dependencies(self):
        """æ£€æŸ¥PyColmapä¾èµ–"""
        try:
            import pycolmap
            self.pycolmap = pycolmap
            print("PyColmap å·²æˆåŠŸå¯¼å…¥")
        except ImportError as e:
            print(f"PyColmap å¯¼å…¥å¤±è´¥: {e}")
            print("è¯·å®‰è£… PyColmap: pip install pycolmap")
            raise ImportError("PyColmap æ˜¯å¿…éœ€çš„ä¾èµ–")

    def estimate_from_images(self, images: List[torch.Tensor], 
                           colmap_feature_type: str = "sift",
                           colmap_matcher_type: str = "sequential", 
                           colmap_quality: str = "medium",
                           enable_dense_reconstruction: bool = False) -> Dict:
        """ä»å›¾ç‰‡åºåˆ—ä¼°è®¡ç›¸æœºå‚æ•°"""
        
        if len(images) < 3:
            return {
                "success": False,
                "error": "å›¾ç‰‡æ•°é‡ä¸è¶³ï¼Œè‡³å°‘éœ€è¦3å¼ å›¾ç‰‡è¿›è¡Œé‡å»º",
                "intrinsics": None,
                "poses": [],
                "statistics": {},
                "frame_count": 0,
                "point_cloud": {}
            }

        # åˆ›å»ºä¸´æ—¶å·¥ä½œç›®å½•
        temp_dir = tempfile.mkdtemp(prefix="colmap_images_")
        images_dir = os.path.join(temp_dir, "images")
        database_path = os.path.join(temp_dir, "database.db")
        output_path = os.path.join(temp_dir, "reconstruction")
        
        try:
            os.makedirs(images_dir, exist_ok=True)
            os.makedirs(output_path, exist_ok=True)

            # 1. ä¿å­˜å›¾ç‰‡åˆ°ä¸´æ—¶ç›®å½•
            print(f"ä¿å­˜ {len(images)} å¼ å›¾ç‰‡åˆ°ä¸´æ—¶ç›®å½•...")
            image_paths = self._save_images_to_temp(images, images_dir)
            
            # 2. è®¾ç½®è´¨é‡å‚æ•°
            quality_settings = self._get_quality_settings(colmap_quality)
            
            # 3. ç‰¹å¾æå–
            print("å¼€å§‹ç‰¹å¾æå–...")
            self._extract_features(database_path, images_dir, quality_settings)
            
            # 4. ç‰¹å¾åŒ¹é…
            print(f"å¼€å§‹ç‰¹å¾åŒ¹é… ({colmap_matcher_type})...")
            self._match_features(database_path, colmap_matcher_type)
            
            # 5. å¢é‡é‡å»º
            print("å¼€å§‹å¢é‡é‡å»º...")
            reconstructions = self._incremental_mapping(database_path, images_dir, output_path)
            
            if not reconstructions:
                return {
                    "success": False,
                    "error": "COLMAP é‡å»ºå¤±è´¥ï¼šæ²¡æœ‰ç”Ÿæˆé‡å»ºç»“æœ",
                    "intrinsics": None,
                    "poses": [],
                    "statistics": {},
                    "frame_count": 0,
                    "point_cloud": {}
                }
            
            # 6. è§£æç»“æœ
            reconstruction = self._get_best_reconstruction(reconstructions)
            result = self._parse_reconstruction(reconstruction, len(images))
            
            print(f"é‡å»ºæˆåŠŸï¼š{len(reconstruction.cameras)} ä¸ªç›¸æœºï¼Œ{len(reconstruction.images)} å¼ å›¾åƒï¼Œ{len(reconstruction.points3D)} ä¸ª3Dç‚¹")
            
            return result
            
        except Exception as e:
            print(f"COLMAP ä¼°è®¡è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            
            return {
                "success": False,
                "error": f"COLMAP é‡å»ºå¤±è´¥: {str(e)}",
                "intrinsics": None,
                "poses": [],
                "statistics": {},
                "frame_count": 0,
                "point_cloud": {}
            }
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                shutil.rmtree(temp_dir)
            except:
                pass

    def _save_images_to_temp(self, images: List[torch.Tensor], images_dir: str) -> List[str]:
        """å°†å›¾ç‰‡å¼ é‡ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•"""
        image_paths = []
        
        for i, image_tensor in enumerate(images):
            # è½¬æ¢å¼ é‡æ ¼å¼
            if image_tensor.dim() == 4:  # Batch dimension
                image_tensor = image_tensor.squeeze(0)
            
            # ç¡®ä¿æ˜¯ HWC æ ¼å¼
            if image_tensor.dim() == 3 and image_tensor.shape[0] == 3:  # CHW -> HWC
                image_tensor = image_tensor.permute(1, 2, 0)
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            image_np = image_tensor.cpu().numpy()
            
            # ç¡®ä¿å€¼åœ¨0-255èŒƒå›´å†…
            if image_np.max() <= 1.0:
                image_np = (image_np * 255).astype(np.uint8)
            else:
                image_np = image_np.astype(np.uint8)
            
            # ä¿å­˜å›¾ç‰‡
            image_filename = f"image_{i:06d}.jpg"
            image_path = os.path.join(images_dir, image_filename)
            
            # è½¬æ¢ä¸ºBGRæ ¼å¼ä¿å­˜ï¼ˆOpenCVæ ¼å¼ï¼‰
            image_bgr = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)
            cv2.imwrite(image_path, image_bgr, [cv2.IMWRITE_JPEG_QUALITY, 95])
            
            image_paths.append(image_path)
        
        return image_paths

    def _get_quality_settings(self, quality: str) -> Dict:
        """è·å–è´¨é‡è®¾ç½®"""
        quality_settings = {
            "low": {"max_image_size": 800, "max_num_features": 4096},
            "medium": {"max_image_size": 1200, "max_num_features": 8192},
            "high": {"max_image_size": 1600, "max_num_features": 16384},
            "extreme": {"max_image_size": 2400, "max_num_features": 32768}
        }
        return quality_settings.get(quality, quality_settings["medium"])

    def _extract_features(self, database_path: str, images_dir: str, quality_settings: Dict):
        """æå–ç‰¹å¾"""
        sift_options = self.pycolmap.SiftExtractionOptions()
        sift_options.max_image_size = quality_settings["max_image_size"]
        sift_options.max_num_features = quality_settings["max_num_features"]
        
        self.pycolmap.extract_features(
            database_path=database_path,
            image_path=images_dir,
            sift_options=sift_options
        )

    def _match_features(self, database_path: str, matcher_type: str):
        """åŒ¹é…ç‰¹å¾"""
        if matcher_type == "exhaustive":
            matching_options = self.pycolmap.ExhaustiveMatchingOptions()
            self.pycolmap.match_exhaustive(
                database_path=database_path,
                matching_options=matching_options
            )
        elif matcher_type == "sequential":
            matching_options = self.pycolmap.SequentialMatchingOptions()
            matching_options.overlap = 10
            self.pycolmap.match_sequential(
                database_path=database_path,
                matching_options=matching_options
            )
        elif matcher_type == "spatial":
            matching_options = self.pycolmap.SpatialMatchingOptions()
            self.pycolmap.match_spatial(
                database_path=database_path,
                matching_options=matching_options
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åŒ¹é…å™¨ç±»å‹: {matcher_type}")

    def _incremental_mapping(self, database_path: str, images_dir: str, output_path: str):
        """å¢é‡é‡å»º"""
        # COLMAP >= 0.3.0 ä½¿ç”¨ IncrementalPipelineOptions ä½œä¸ºé…ç½®ç±»å‹
        # æ—©æœŸç‰ˆæœ¬å¯èƒ½ä»æ”¯æŒ IncrementalMapperOptionsï¼Œä½†ä¸ºä¿æŒå…¼å®¹æ€§
        # è¿™é‡Œä¼˜å…ˆå°è¯• IncrementalPipelineOptionsï¼Œå¹¶åœ¨å›é€€æƒ…å†µä¸‹ä½¿ç”¨é»˜è®¤é…ç½®ã€‚

        try:
            pipeline_options = self.pycolmap.IncrementalPipelineOptions()

            # é€šè¿‡å†…éƒ¨çš„ mapper_options è®¾ç½®å¸¸ç”¨å‚æ•°
            if hasattr(pipeline_options, "mapper_options"):
                mapper_opts = pipeline_options.mapper_options
                mapper_opts.ba_refine_focal_length = True
                mapper_opts.ba_refine_principal_point = True
                mapper_opts.init_min_num_inliers = 100
                mapper_opts.init_max_reg_trials = 2
        except AttributeError:
            # å¦‚æœå®‰è£…çš„ pycolmap ç‰ˆæœ¬æ²¡æœ‰ IncrementalPipelineOptionsï¼Œåˆ™é€€å›é»˜è®¤
            pipeline_options = None

        if pipeline_options is not None:
            reconstructions = self.pycolmap.incremental_mapping(
                database_path=database_path,
                image_path=images_dir,
                output_path=output_path,
                options=pipeline_options
            )
        else:
            # å›é€€ï¼šä¸æ˜¾å¼ä¼ å…¥ optionsï¼Œä½¿ç”¨åº“é»˜è®¤å€¼ã€‚
            reconstructions = self.pycolmap.incremental_mapping(
                database_path=database_path,
                image_path=images_dir,
                output_path=output_path
            )
        
        return reconstructions

    def _get_best_reconstruction(self, reconstructions):
        """è·å–æœ€ä½³é‡å»ºç»“æœ"""
        if isinstance(reconstructions, dict):
            if len(reconstructions) == 0:
                raise RuntimeError("é‡å»ºç»“æœä¸ºç©º")
            # é€‰æ‹©å›¾åƒæ•°é‡æœ€å¤šçš„é‡å»º
            best_recon = max(reconstructions.values(), key=lambda r: len(r.images))
            return best_recon
        elif isinstance(reconstructions, list):
            if len(reconstructions) == 0:
                raise RuntimeError("é‡å»ºç»“æœä¸ºç©º")
            # é€‰æ‹©å›¾åƒæ•°é‡æœ€å¤šçš„é‡å»º
            best_recon = max(reconstructions, key=lambda r: len(r.images))
            return best_recon
        else:
            return reconstructions

    def _parse_reconstruction(self, reconstruction, num_input_images: int) -> Dict:
        """è§£æé‡å»ºç»“æœ"""
        try:
            # è§£æç›¸æœºå†…å‚
            intrinsics = self._parse_camera_intrinsics(reconstruction)
            
            # è§£æä½å§¿
            poses = self._parse_camera_poses(reconstruction)
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            statistics = self._calculate_statistics(poses, reconstruction)
            
            # ç‚¹äº‘ä¿¡æ¯
            point_cloud_info = {
                "num_points": len(reconstruction.points3D),
                "num_cameras": len(reconstruction.cameras),
                "num_registered_images": len(reconstruction.images),
                "num_input_images": num_input_images,
                "registration_ratio": len(reconstruction.images) / max(num_input_images, 1)
            }
            
            return {
                "success": True,
                "intrinsics": intrinsics,
                "poses": poses,
                "statistics": statistics,
                "frame_count": len(poses),
                "point_cloud": point_cloud_info
            }
            
        except Exception as e:
            print(f"è§£æé‡å»ºç»“æœå¤±è´¥: {e}")
            return {
                "success": False,
                "error": f"è§£æå¤±è´¥: {str(e)}",
                "intrinsics": None,
                "poses": [],
                "statistics": {},
                "frame_count": 0,
                "point_cloud": {}
            }

    def _parse_camera_intrinsics(self, reconstruction) -> Dict:
        """è§£æç›¸æœºå†…å‚"""
        if len(reconstruction.cameras) == 0:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°ç›¸æœºå‚æ•°")
        
        # è·å–ç¬¬ä¸€ä¸ªç›¸æœºçš„å‚æ•°ï¼ˆå‡è®¾æ‰€æœ‰å›¾åƒä½¿ç”¨åŒä¸€ç›¸æœºï¼‰
        camera = list(reconstruction.cameras.values())[0]
        
        # è·å–åŸºæœ¬å‚æ•°
        focal_length = float(camera.params[0]) if len(camera.params) > 0 else 800.0
        focal_length_y = float(camera.params[1]) if len(camera.params) > 1 else focal_length
        
        # è·å–ä¸»ç‚¹
        if len(camera.params) > 3:
            principal_point = [float(camera.params[2]), float(camera.params[3])]
        else:
            principal_point = [camera.width / 2, camera.height / 2]
        
        # è·å–ç•¸å˜å‚æ•°
        distortion = [float(p) for p in camera.params[4:]] if len(camera.params) > 4 else []
        
        # è·å–ç›¸æœºæ¨¡å‹åç§°
        camera_model = "PINHOLE"  # é»˜è®¤å€¼
        if hasattr(camera, 'model_name'):
            camera_model = camera.model_name
        elif hasattr(camera, 'model_id'):
            model_names = {
                0: "SIMPLE_PINHOLE", 1: "PINHOLE", 2: "SIMPLE_RADIAL",
                3: "RADIAL", 4: "OPENCV", 5: "OPENCV_FISHEYE"
            }
            camera_model = model_names.get(camera.model_id, f"MODEL_{camera.model_id}")
        
        return {
            "focal_length": focal_length,
            "focal_length_y": focal_length_y,
            "principal_point": principal_point,
            "image_size": [camera.width, camera.height],
            "camera_model": camera_model,
            "distortion": distortion
        }

    def _parse_camera_poses(self, reconstruction) -> List[Dict]:
        """è§£æç›¸æœºä½å§¿"""
        poses = []
        
        for image_id, image in reconstruction.images.items():
            # è·å–ä½å§¿æ•°æ® (å…¼å®¹ä¸åŒç‰ˆæœ¬çš„ pycolmap)
            quat = None
            trans = None
            
            # é¦–æ¬¡è¿­ä»£æ—¶æ‰“å°å¯ç”¨å±æ€§ä»¥ä¾¿è°ƒè¯•
            if image_id == list(reconstruction.images.keys())[0]:
                print(f"PyColmap Image å¯¹è±¡å¯ç”¨å±æ€§: {[attr for attr in dir(image) if not attr.startswith('_')]}")
            
            # å°è¯•å¤šç§å¯èƒ½çš„API
            if hasattr(image, "qvec") and hasattr(image, "tvec"):
                # æ—§ç‰ˆ API: ç›´æ¥è®¿é—®å±æ€§
                quat = image.qvec.tolist() if hasattr(image.qvec, 'tolist') else list(image.qvec)
                trans = image.tvec.tolist() if hasattr(image.tvec, 'tolist') else list(image.tvec)
            elif hasattr(image, "cam_from_world"):
                # æ–°ç‰ˆ API: ä½¿ç”¨ cam_from_world å±æ€§
                # cam_from_world æ˜¯ä¸€ä¸ª Rigid3d å¯¹è±¡ï¼ŒåŒ…å«æ—‹è½¬å’Œå¹³ç§»
                cam_from_world = image.cam_from_world
                if hasattr(cam_from_world, "rotation"):
                    # è·å–å››å…ƒæ•°
                    rotation = cam_from_world.rotation
                    if hasattr(rotation, "quat"):
                        quat = rotation.quat.tolist()
                    elif hasattr(rotation, "quaternion"):
                        quat = rotation.quaternion.tolist()
                    else:
                        # å°è¯•ä»æ—‹è½¬çŸ©é˜µè½¬æ¢
                        print(f"è­¦å‘Š: æ— æ³•ç›´æ¥è·å–å››å…ƒæ•°ï¼Œå°è¯•å…¶ä»–æ–¹æ³•")
                
                if hasattr(cam_from_world, "translation"):
                    trans = cam_from_world.translation.tolist()
            elif hasattr(image, "projection_center"):
                # å¦ä¸€ç§å¯èƒ½: ä½¿ç”¨ projection_center ä½œä¸ºä½ç½®
                print(f"ä½¿ç”¨ projection_center ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ")
                trans = image.projection_center().tolist()
                # å°è¯•è·å–æ—‹è½¬
                if hasattr(image, "rotation_matrix"):
                    # ä»æ—‹è½¬çŸ©é˜µè®¡ç®—å››å…ƒæ•°
                    print(f"ä» rotation_matrix è®¡ç®—å››å…ƒæ•°")
            
            # å¦‚æœä»ç„¶æ— æ³•è·å–æ•°æ®ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
            if quat is None or trans is None:
                print(f"è­¦å‘Š: æ— æ³•è·å–å›¾åƒ {image_id} çš„å®Œæ•´ä½å§¿æ•°æ®")
                # æä¾›é»˜è®¤å€¼ä»¥é¿å…å´©æºƒ
                if quat is None:
                    quat = [1.0, 0.0, 0.0, 0.0]  # å•ä½å››å…ƒæ•°
                if trans is None:
                    trans = [0.0, 0.0, 0.0]  # åŸç‚¹
            
            # è½¬æ¢ä¸ºæ¬§æ‹‰è§’
            euler = self._quaternion_to_euler(quat)
            
            # è·å–å›¾åƒåç§°
            image_name = image.name if hasattr(image, 'name') else f"image_{image_id}"
            
            pose = {
                "position": trans,
                "rotation_quaternion": quat,
                "rotation_euler": euler,
                "image_name": image_name,
                "image_id": image_id
            }
            poses.append(pose)
        
        # æŒ‰å›¾åƒåç§°æ’åºï¼Œç¡®ä¿é¡ºåº
        poses.sort(key=lambda x: x["image_name"])
        
        return poses

    def _quaternion_to_euler(self, quat: List[float]) -> List[float]:
        """å››å…ƒæ•°è½¬æ¬§æ‹‰è§’"""
        qw, qx, qy, qz = quat
        
        # Roll (x-axis rotation)
        sinr_cosp = 2 * (qw * qx + qy * qz)
        cosr_cosp = 1 - 2 * (qx * qx + qy * qy)
        roll = np.arctan2(sinr_cosp, cosr_cosp)
        
        # Pitch (y-axis rotation)
        sinp = 2 * (qw * qy - qz * qx)
        if abs(sinp) >= 1:
            pitch = np.copysign(np.pi / 2, sinp)
        else:
            pitch = np.arcsin(sinp)
        
        # Yaw (z-axis rotation)
        siny_cosp = 2 * (qw * qz + qx * qy)
        cosy_cosp = 1 - 2 * (qy * qy + qz * qz)
        yaw = np.arctan2(siny_cosp, cosy_cosp)
        
        return [roll, pitch, yaw]

    def _calculate_statistics(self, poses: List[Dict], reconstruction) -> Dict:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        if len(poses) < 2:
            return {
                "total_distance": 0.0,
                "average_speed": 0.0,
                "num_poses": len(poses),
                "num_3d_points": len(reconstruction.points3D)
            }
        
        # è®¡ç®—è½¨è¿¹æ€»é•¿åº¦
        total_distance = 0.0
        for i in range(1, len(poses)):
            pos1 = np.array(poses[i-1]["position"])
            pos2 = np.array(poses[i]["position"])
            distance = np.linalg.norm(pos2 - pos1)
            total_distance += distance
        
        average_speed = total_distance / (len(poses) - 1) if len(poses) > 1 else 0.0
        
        return {
            "total_distance": float(total_distance),
            "average_speed": float(average_speed),
            "num_poses": len(poses),
            "num_3d_points": len(reconstruction.points3D),
            "mean_track_length": np.mean([len(point.track.elements) for point in reconstruction.points3D.values()]) if len(reconstruction.points3D) > 0 else 0.0
        }


class ImageSequenceCameraEstimator:
    """å›¾ç‰‡åºåˆ—ç›¸æœºå‚æ•°ä¼°è®¡èŠ‚ç‚¹"""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "images": ("IMAGE", {
                    "tooltip": "è¾“å…¥å›¾ç‰‡åºåˆ—"
                }),
            },
            "optional": {
                "colmap_feature_type": (["sift", "superpoint", "disk"], {
                    "default": "sift",
                    "tooltip": "COLMAPç‰¹å¾æ£€æµ‹å™¨ç±»å‹ï¼ŒSIFTæœ€ç¨³å®š"
                }),
                "colmap_matcher_type": (["exhaustive", "sequential", "spatial"], {
                    "default": "sequential",
                    "tooltip": "COLMAPåŒ¹é…ç­–ç•¥ï¼Œsequentialé€‚åˆæœ‰åºåºåˆ—"
                }),
                "colmap_quality": (["low", "medium", "high", "extreme"], {
                    "default": "medium",
                    "tooltip": "COLMAPé‡å»ºè´¨é‡ï¼Œhigherè´¨é‡éœ€è¦æ›´å¤šæ—¶é—´"
                }),
                "enable_dense_reconstruction": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "æ˜¯å¦å¯ç”¨å¯†é›†é‡å»ºï¼ˆéœ€è¦æ›´å¤šè®¡ç®—èµ„æºå’ŒCUDAæ”¯æŒï¼‰"
                }),
                "enable_visualization": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "æ˜¯å¦ç”Ÿæˆè½¨è¿¹å¯è§†åŒ–å›¾åƒ"
                }),
                "output_format": (["json", "detailed_json"], {
                    "default": "detailed_json",
                    "tooltip": "è¾“å‡ºæ ¼å¼ï¼Œè¯¦ç»†æ¨¡å¼åŒ…å«æ›´å¤šç»Ÿè®¡ä¿¡æ¯"
                })
            }
        }

    RETURN_TYPES = ("STRING", "IMAGE", "STRING", "STRING", "STRING")
    RETURN_NAMES = ("intrinsics_json", "trajectory_visualization", "poses_json", "statistics_json", "point_cloud_info")
    FUNCTION = "estimate_camera_parameters"
    CATEGORY = "ğŸ’ƒVVL/VideoCamera"

    def __init__(self):
        try:
            self.estimator = ColmapCameraEstimator()
        except ImportError as e:
            print(f"COLMAPåˆå§‹åŒ–å¤±è´¥: {e}")
            self.estimator = None

    def estimate_camera_parameters(self, images, 
                                 colmap_feature_type: str = "sift",
                                 colmap_matcher_type: str = "sequential", 
                                 colmap_quality: str = "medium",
                                 enable_dense_reconstruction: bool = False,
                                 enable_visualization: bool = True,
                                 output_format: str = "detailed_json") -> tuple:
        """ä»å›¾ç‰‡åºåˆ—ä¼°è®¡ç›¸æœºå‚æ•°çš„ä¸»å‡½æ•°"""
        
        try:
            if self.estimator is None:
                raise RuntimeError("COLMAPåˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥PyColmapå®‰è£…")
            
            # æ£€æŸ¥è¾“å…¥
            if images is None:
                raise ValueError("æœªæä¾›å›¾ç‰‡è¾“å…¥")
            
            # è½¬æ¢è¾“å…¥æ ¼å¼
            if isinstance(images, torch.Tensor):
                if images.dim() == 4:  # Batch of images
                    image_list = [images[i] for i in range(images.shape[0])]
                else:  # Single image
                    image_list = [images]
            elif isinstance(images, list):
                image_list = images
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å›¾ç‰‡è¾“å…¥æ ¼å¼: {type(images)}")
            
            print(f"å¼€å§‹å¤„ç† {len(image_list)} å¼ å›¾ç‰‡")
            
            # ä½¿ç”¨COLMAPè¿›è¡Œä¼°è®¡
            result = self.estimator.estimate_from_images(
                images=image_list,
                colmap_feature_type=colmap_feature_type,
                colmap_matcher_type=colmap_matcher_type,
                colmap_quality=colmap_quality,
                enable_dense_reconstruction=enable_dense_reconstruction
            )
            
            if not result["success"]:
                raise RuntimeError(f"ç›¸æœºå‚æ•°ä¼°è®¡å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
            # å‡†å¤‡è¾“å‡º
            intrinsics_json = self._format_intrinsics_output(result["intrinsics"], output_format)
            poses_json = self._format_poses_output(result["poses"], output_format)
            statistics_json = self._format_statistics_output(result["statistics"], result, output_format)
            point_cloud_info = self._format_point_cloud_output(result["point_cloud"], output_format)
            
            # å¤„ç†å¯è§†åŒ–å›¾åƒ
            if enable_visualization and result["poses"]:
                trajectory_img = self._create_trajectory_visualization(result["poses"])
            else:
                trajectory_img = self._create_empty_visualization()
            
            print(f"æˆåŠŸå¤„ç† {result['frame_count']} å¼ å›¾ç‰‡")
            if result['intrinsics']:
                print(f"ä¼°è®¡çš„ç„¦è·: {result['intrinsics']['focal_length']:.2f}")
            
            return (intrinsics_json, trajectory_img, poses_json, statistics_json, point_cloud_info)
            
        except Exception as e:
            error_msg = f"å›¾ç‰‡åºåˆ—ç›¸æœºå‚æ•°ä¼°è®¡å‡ºé”™: {str(e)}"
            print(error_msg)
            
            # è¿”å›é”™è¯¯ä¿¡æ¯
            error_json = json.dumps({"error": error_msg, "success": False}, ensure_ascii=False, indent=2)
            empty_img = self._create_empty_visualization()
            
            return (error_json, empty_img, error_json, error_json, error_json)

    def _format_intrinsics_output(self, intrinsics: Dict, output_format: str) -> str:
        """æ ¼å¼åŒ–å†…å‚è¾“å‡º"""
        if output_format == "json":
            simplified = {
                "focal_length": intrinsics["focal_length"],
                "principal_point": intrinsics["principal_point"],
                "image_size": intrinsics["image_size"]
            }
            return json.dumps(simplified, ensure_ascii=False, indent=2)
        else:
            return json.dumps(intrinsics, ensure_ascii=False, indent=2)

    def _format_poses_output(self, poses: List[Dict], output_format: str) -> str:
        """æ ¼å¼åŒ–ä½å§¿è¾“å‡º"""
        if output_format == "json":
            simplified_poses = []
            for i, pose in enumerate(poses):
                simplified_poses.append({
                    "frame": i,
                    "position": pose["position"],
                    "rotation_euler": pose["rotation_euler"]
                })
            return json.dumps(simplified_poses, ensure_ascii=False, indent=2)
        else:
            detailed_poses = []
            for i, pose in enumerate(poses):
                detailed_pose = pose.copy()
                detailed_pose["frame"] = i
                detailed_poses.append(detailed_pose)
            return json.dumps(detailed_poses, ensure_ascii=False, indent=2)

    def _format_statistics_output(self, statistics: Dict, result: Dict, output_format: str) -> str:
        """æ ¼å¼åŒ–ç»Ÿè®¡ä¿¡æ¯è¾“å‡º"""
        if output_format == "json":
            simplified_stats = {
                "frame_count": result["frame_count"],
                "total_distance": statistics.get("total_distance", 0),
                "num_3d_points": statistics.get("num_3d_points", 0),
                "success": result["success"]
            }
            return json.dumps(simplified_stats, ensure_ascii=False, indent=2)
        else:
            detailed_stats = statistics.copy()
            detailed_stats.update({
                "frame_count": result["frame_count"],
                "success": result["success"]
            })
            return json.dumps(detailed_stats, ensure_ascii=False, indent=2)

    def _format_point_cloud_output(self, point_cloud_info: Dict, output_format: str) -> str:
        """æ ¼å¼åŒ–ç‚¹äº‘ä¿¡æ¯è¾“å‡º"""
        if output_format == "json":
            simplified = {
                "num_points": point_cloud_info.get("num_points", 0),
                "registration_ratio": point_cloud_info.get("registration_ratio", 0)
            }
            return json.dumps(simplified, ensure_ascii=False, indent=2)
        else:
            return json.dumps(point_cloud_info, ensure_ascii=False, indent=2)

    def _create_trajectory_visualization(self, poses: List[Dict]) -> torch.Tensor:
        """åˆ›å»ºè½¨è¿¹å¯è§†åŒ–"""
        try:
            import matplotlib.pyplot as plt
            from mpl_toolkits.mplot3d import Axes3D
            
            # æå–ä½ç½®ä¿¡æ¯
            positions = np.array([pose["position"] for pose in poses])
            
            fig = plt.figure(figsize=(12, 9))
            ax = fig.add_subplot(111, projection='3d')
            
            # ç»˜åˆ¶è½¨è¿¹çº¿
            if len(positions) > 1:
                ax.plot(positions[:, 0], positions[:, 1], positions[:, 2], 
                       'b-', linewidth=3, alpha=0.8, label='Camera Path')
            
            # ç”¨é¢œè‰²æ¸å˜è¡¨ç¤ºæ—¶é—´è¿›ç¨‹
            colors = plt.cm.viridis(np.linspace(0, 1, len(positions)))
            scatter = ax.scatter(positions[:, 0], positions[:, 1], positions[:, 2], 
                               c=colors, s=60, alpha=0.9, edgecolors='black', linewidth=0.5)
            
            # æ ‡è®°èµ·ç‚¹å’Œç»ˆç‚¹
            if len(positions) > 0:
                ax.scatter([positions[0, 0]], [positions[0, 1]], [positions[0, 2]], 
                          c='green', s=150, marker='^', label='Start', edgecolors='darkgreen', linewidth=2)
            if len(positions) > 1:
                ax.scatter([positions[-1, 0]], [positions[-1, 1]], [positions[-1, 2]], 
                          c='red', s=150, marker='o', label='End', edgecolors='darkred', linewidth=2)
            
            # è®¾ç½®åæ ‡è½´
            ax.set_xlabel('X (meters)', fontsize=12)
            ax.set_ylabel('Y (meters)', fontsize=12)
            ax.set_zlabel('Z (meters)', fontsize=12)
            ax.set_title('COLMAP Camera Trajectory (3D View)', fontsize=14, fontweight='bold')
            
            # æ·»åŠ å›¾ä¾‹å’Œé¢œè‰²æ¡
            ax.legend(loc='upper right', fontsize=10)
            cbar = plt.colorbar(scatter, ax=ax, shrink=0.6, aspect=20)
            cbar.set_label('Time Progress', fontsize=10)
            
            # è®¾ç½®ç›¸ç­‰çš„åæ ‡è½´æ¯”ä¾‹
            max_range = np.array([positions.max(axis=0) - positions.min(axis=0)]).max() / 2.0
            mid_x = (positions.max(axis=0)[0] + positions.min(axis=0)[0]) * 0.5
            mid_y = (positions.max(axis=0)[1] + positions.min(axis=0)[1]) * 0.5
            mid_z = (positions.max(axis=0)[2] + positions.min(axis=0)[2]) * 0.5
            
            ax.set_xlim(mid_x - max_range, mid_x + max_range)
            ax.set_ylim(mid_y - max_range, mid_y + max_range)
            ax.set_zlim(mid_z - max_range, mid_z + max_range)
            
            ax.grid(True, alpha=0.3)
            ax.view_init(elev=20, azim=45)
            
            # ä¿å­˜ä¸ºå›¾åƒ
            with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
                plt.savefig(tmp.name, dpi=120, bbox_inches='tight', facecolor='white')
                plt.close()
                
                # è¯»å–å›¾åƒ
                img = cv2.imread(tmp.name)
                os.unlink(tmp.name)
                
                if img is not None:
                    # BGRè½¬RGB
                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                    img_tensor = torch.from_numpy(img.astype(np.float32) / 255.0)
                    return img_tensor.unsqueeze(0)
                else:
                    return self._create_empty_visualization()
        
        except Exception as e:
            print(f"åˆ›å»ºå¯è§†åŒ–å¤±è´¥: {e}")
            return self._create_empty_visualization()

    def _create_empty_visualization(self) -> torch.Tensor:
        """åˆ›å»ºç©ºçš„å¯è§†åŒ–å›¾åƒ"""
        img = np.ones((400, 600, 3), dtype=np.float32) * 0.1
        
        try:
            img_uint8 = (img * 255).astype(np.uint8)
            cv2.putText(img_uint8, "No Trajectory", (180, 180), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 2)
            cv2.putText(img_uint8, "Visualization", (170, 220), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 2)
            img = img_uint8.astype(np.float32) / 255.0
        except:
            pass
        
        return torch.from_numpy(img).unsqueeze(0)

# èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "ImageSequenceCameraEstimator": ImageSequenceCameraEstimator
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "ImageSequenceCameraEstimator": "VVL Image Sequence Camera Estimator"
} 