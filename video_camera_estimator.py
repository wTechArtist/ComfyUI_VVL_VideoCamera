import torch
import numpy as np
import json
import os
import tempfile
import cv2
import shutil
from PIL import Image
from typing import List, Dict, Any
import pathlib

# æ·»åŠ ComfyUIç±»å‹å¯¼å…¥
try:
    from comfy.comfy_types import IO
except ImportError:
    # å¦‚æœæ— æ³•å¯¼å…¥ï¼Œåˆ›å»ºä¸€ä¸ªå…¼å®¹çš„ç±»
    class IO:
        IMAGE = "IMAGE"

class ColmapCameraEstimator:
    """ä½¿ç”¨åŸç”ŸCOLMAPè¿›è¡Œç›¸æœºå‚æ•°ä¼°è®¡çš„æ ¸å¿ƒç±»"""

    def __init__(self):
        self.check_dependencies()

    def check_dependencies(self):
        """æ£€æŸ¥åŸç”ŸCOLMAPä¾èµ–ï¼ˆå¿…éœ€ï¼‰"""
        # æ£€æŸ¥åŸç”ŸCOLMAPæ˜¯å¦å¯ç”¨ï¼ˆå¿…éœ€ï¼‰
        try:
            import subprocess
            result = subprocess.run(['colmap', 'help'], capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                self.colmap_available = True
                print("âœ“ åŸç”ŸCOLMAP å·²æˆåŠŸæ£€æµ‹åˆ°")
                
                # æ£€æµ‹COLMAPç‰ˆæœ¬å’ŒGPUæ”¯æŒ
                self._detect_colmap_capabilities()
            else:
                raise RuntimeError("åŸç”ŸCOLMAPå‘½ä»¤æ‰§è¡Œå¤±è´¥")
        except (subprocess.TimeoutExpired, FileNotFoundError, Exception) as e:
            raise RuntimeError(f"æœªæ‰¾åˆ°åŸç”ŸCOLMAPã€‚è¯·å®‰è£…COLMAPå¹¶ç¡®ä¿å…¶åœ¨ç³»ç»ŸPATHä¸­ã€‚é”™è¯¯: {e}")
        
        # æ£€æŸ¥PyColmapï¼ˆä»…ç”¨äºè¯»å–ç»“æœæ–‡ä»¶ï¼‰
        try:
            import pycolmap
            self.pycolmap = pycolmap
            print("âœ“ PyColmap å¯ç”¨äºè¯»å–COLMAPç»“æœæ–‡ä»¶")
        except ImportError as e:
            raise ImportError(f"PyColmap æ˜¯å¿…éœ€çš„ä¾èµ–ï¼ˆç”¨äºè¯»å–COLMAPç»“æœï¼‰: {e}")

    def _detect_colmap_capabilities(self):
        """æ£€æµ‹COLMAPçš„ç‰ˆæœ¬å’ŒåŠŸèƒ½"""
        import subprocess
        
        # æ£€æµ‹ç‰ˆæœ¬
        try:
            result = subprocess.run(['colmap'], capture_output=True, text=True, timeout=5)
            self.colmap_version = "unknown"
            if "COLMAP" in result.stderr:
                # å°è¯•æå–ç‰ˆæœ¬ä¿¡æ¯
                for line in result.stderr.split('\n'):
                    if 'COLMAP' in line and ('3.' in line or '4.' in line):
                        self.colmap_version = line.strip()
                        break
            print(f"COLMAPç‰ˆæœ¬: {self.colmap_version}")
        except:
            self.colmap_version = "unknown"
        
        # æ£€æµ‹GPUæ”¯æŒ
        self.gpu_available = self._check_gpu_support()
        print(f"GPUæ”¯æŒ: {'å¯ç”¨' if self.gpu_available else 'ä¸å¯ç”¨'}")

    def _check_gpu_support(self) -> bool:
        """æ£€æµ‹GPUæ”¯æŒ"""
        try:
            import subprocess
            # å°è¯•è¿è¡Œç®€å•çš„GPUæµ‹è¯•
            result = subprocess.run([
                'colmap', 'feature_extractor', '--help'
            ], capture_output=True, text=True, timeout=5)
            
            # æ£€æŸ¥å¸®åŠ©ä¿¡æ¯ä¸­æ˜¯å¦åŒ…å«GPUç›¸å…³é€‰é¡¹
            return 'gpu' in result.stdout.lower() or 'cuda' in result.stdout.lower()
        except:
            return False

    def estimate_from_images(self, images: List[torch.Tensor], 
                           colmap_feature_type: str = "sift",
                           colmap_matcher_type: str = "sequential", 
                           colmap_quality: str = "medium",
                           enable_dense_reconstruction: bool = False,
                           force_gpu: bool = True) -> Dict:
        """ä½¿ç”¨åŸç”ŸCOLMAPä»å›¾ç‰‡åºåˆ—ä¼°è®¡ç›¸æœºå‚æ•°"""
        
        if len(images) < 3:
            return {
                "success": False,
                "error": "å›¾ç‰‡æ•°é‡ä¸è¶³ï¼Œè‡³å°‘éœ€è¦3å¼ å›¾ç‰‡è¿›è¡Œé‡å»º",
                "intrinsics": None,
                "poses": [],
                "statistics": {},
                "frame_count": 0,
                "point_cloud": {}
            }

        print("ä½¿ç”¨åŸç”ŸCOLMAPè¿›è¡Œç›¸æœºå‚æ•°ä¼°è®¡...")
        print(f"å¼ºåˆ¶GPUæ¨¡å¼: {'æ˜¯' if force_gpu else 'å¦'}")
        
        return self._estimate_with_native_colmap(
            images, colmap_feature_type, colmap_matcher_type, 
            colmap_quality, enable_dense_reconstruction, force_gpu
        )

    def _estimate_with_native_colmap(self, images: List[torch.Tensor], 
                                   colmap_feature_type: str,
                                   colmap_matcher_type: str, 
                                   colmap_quality: str,
                                   enable_dense_reconstruction: bool,
                                   force_gpu: bool) -> Dict:
        """ä½¿ç”¨åŸç”ŸCOLMAPè¿›è¡Œä¼°è®¡"""
        
        import subprocess
        
        # åˆ›å»ºä¸´æ—¶å·¥ä½œç›®å½•
        temp_dir = tempfile.mkdtemp(prefix="colmap_native_")
        images_dir = os.path.join(temp_dir, "images")
        database_path = os.path.join(temp_dir, "database.db")
        sparse_dir = os.path.join(temp_dir, "sparse")
        
        try:
            os.makedirs(images_dir, exist_ok=True)
            os.makedirs(sparse_dir, exist_ok=True)

            # 1. ä¿å­˜å›¾ç‰‡åˆ°ä¸´æ—¶ç›®å½•
            print(f"ä¿å­˜ {len(images)} å¼ å›¾ç‰‡åˆ°ä¸´æ—¶ç›®å½•...")
            image_paths = self._save_images_to_temp(images, images_dir)
            
            # 2. è®¾ç½®è´¨é‡å‚æ•°
            quality_settings = self._get_quality_settings(colmap_quality)
            
            # 3. ç‰¹å¾æå– (åŸç”ŸCOLMAPå‘½ä»¤)
            print("å¼€å§‹åŸç”ŸCOLMAPç‰¹å¾æå–...")
            self._run_native_feature_extraction(database_path, images_dir, quality_settings, force_gpu)
            
            # 4. ç‰¹å¾åŒ¹é… (åŸç”ŸCOLMAPå‘½ä»¤)
            print(f"å¼€å§‹åŸç”ŸCOLMAPç‰¹å¾åŒ¹é… ({colmap_matcher_type})...")
            self._run_native_feature_matching(database_path, colmap_matcher_type, force_gpu)
            
            # 5. å¢é‡é‡å»º (åŸç”ŸCOLMAPå‘½ä»¤)
            print("å¼€å§‹åŸç”ŸCOLMAPå¢é‡é‡å»º...")
            self._run_native_mapping(database_path, images_dir, sparse_dir, force_gpu)
            
            # 6. è¯»å–é‡å»ºç»“æœ
            reconstruction = self._read_native_reconstruction(sparse_dir)
            
            if reconstruction is None:
                return {
                    "success": False,
                    "error": "åŸç”ŸCOLMAP é‡å»ºå¤±è´¥ï¼šæ²¡æœ‰ç”Ÿæˆé‡å»ºç»“æœ",
                    "intrinsics": None,
                    "poses": [],
                    "statistics": {},
                    "frame_count": 0,
                    "point_cloud": {}
                }
            
            # 7. è§£æç»“æœ
            result = self._parse_native_reconstruction(reconstruction, len(images))
            
            print(f"åŸç”ŸCOLMAPé‡å»ºæˆåŠŸï¼šæ³¨å†Œäº† {len(result['poses'])} å¼ å›¾åƒ")
            
            return result
            
        except Exception as e:
            print(f"åŸç”ŸCOLMAP ä¼°è®¡è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            
            return {
                "success": False,
                "error": f"åŸç”ŸCOLMAP é‡å»ºå¤±è´¥: {str(e)}",
                "intrinsics": None,
                "poses": [],
                "statistics": {},
                "frame_count": 0,
                "point_cloud": {}
            }
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                shutil.rmtree(temp_dir)
            except:
                pass

    def _save_images_to_temp(self, images: List[torch.Tensor], images_dir: str) -> List[str]:
        """å°†å›¾ç‰‡å¼ é‡ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•"""
        image_paths = []
        
        for i, image_tensor in enumerate(images):
            # è½¬æ¢å¼ é‡æ ¼å¼
            if image_tensor.dim() == 4:  # Batch dimension
                image_tensor = image_tensor.squeeze(0)
            
            # ç¡®ä¿æ˜¯ HWC æ ¼å¼
            if image_tensor.dim() == 3 and image_tensor.shape[0] == 3:  # CHW -> HWC
                image_tensor = image_tensor.permute(1, 2, 0)
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            image_np = image_tensor.cpu().numpy()
            
            # ç¡®ä¿å€¼åœ¨0-255èŒƒå›´å†…
            if image_np.max() <= 1.0:
                image_np = (image_np * 255).astype(np.uint8)
            else:
                image_np = image_np.astype(np.uint8)
            
            # ä¿å­˜å›¾ç‰‡
            image_filename = f"image_{i:06d}.jpg"
            image_path = os.path.join(images_dir, image_filename)
            
            # è½¬æ¢ä¸ºBGRæ ¼å¼ä¿å­˜ï¼ˆOpenCVæ ¼å¼ï¼‰
            image_bgr = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)
            cv2.imwrite(image_path, image_bgr, [cv2.IMWRITE_JPEG_QUALITY, 95])
            
            image_paths.append(image_path)
        
        return image_paths

    def _get_quality_settings(self, quality: str) -> Dict:
        """è·å–è´¨é‡è®¾ç½®"""
        quality_settings = {
            "low": {"max_image_size": 800, "max_num_features": 4096},
            "medium": {"max_image_size": 1200, "max_num_features": 8192},
            "high": {"max_image_size": 1600, "max_num_features": 16384},
            "extreme": {"max_image_size": 2400, "max_num_features": 32768}
        }
        return quality_settings.get(quality, quality_settings["medium"])

    def _setup_gpu_environment(self) -> Dict[str, str]:
        """è®¾ç½®GPUç¯å¢ƒå˜é‡"""
        import os
        
        env = os.environ.copy()
        
        # å¼ºåˆ¶ä½¿ç”¨GPU
        env['CUDA_VISIBLE_DEVICES'] = '0'  # ä½¿ç”¨ç¬¬ä¸€ä¸ªGPU
        
        # è®¾ç½®OpenGL/EGLç›¸å…³å˜é‡ä»¥æ”¯æŒæ— å¤´GPUæ“ä½œ
        env['__GL_SYNC_TO_VBLANK'] = '0'
        env['__GL_ALLOW_UNOFFICIAL_PROTOCOL'] = '1'
        
        # å°è¯•ä½¿ç”¨EGLè€Œä¸æ˜¯GLX
        env['__EGL_VENDOR_LIBRARY_DIRS'] = '/usr/share/glvnd/egl_vendor.d'
        
        # ç¦ç”¨Qtçš„XCBæ’ä»¶ï¼Œä½¿ç”¨offscreen
        env['QT_QPA_PLATFORM'] = 'offscreen'
        env['QT_QPA_FONTDIR'] = '/usr/share/fonts'
        
        # NVIDIAç›¸å…³è®¾ç½®
        env['NVIDIA_VISIBLE_DEVICES'] = 'all'
        env['NVIDIA_DRIVER_CAPABILITIES'] = 'compute,utility,graphics'
        
        return env

    def _run_native_feature_extraction(self, database_path: str, images_dir: str, quality_settings: Dict, force_gpu: bool = True):
        """è¿è¡ŒåŸç”ŸCOLMAPç‰¹å¾æå–"""
        import subprocess
        import os
        
        if force_gpu and self.gpu_available:
            print("ğŸš€ ä½¿ç”¨å¼ºåˆ¶GPUæ¨¡å¼è¿è¡ŒCOLMAPç‰¹å¾æå–...")
            
            # è®¾ç½®GPUç¯å¢ƒ
            env = self._setup_gpu_environment()
            
            # GPUæ¨¡å¼å‘½ä»¤ - ä½¿ç”¨æ›´ç®€æ´çš„å‚æ•°
            gpu_cmd = [
                'colmap', 'feature_extractor',
                '--database_path', database_path,
                '--image_path', images_dir,
                '--ImageReader.single_camera', '1',
                '--SiftExtraction.use_gpu', '1',
                '--SiftExtraction.gpu_index', '0',  # æ˜ç¡®æŒ‡å®šGPU 0
                '--SiftExtraction.max_image_size', str(quality_settings["max_image_size"]),
                '--SiftExtraction.max_num_features', str(quality_settings["max_num_features"])
            ]
            
            # é¦–å…ˆå°è¯•ç›´æ¥GPUæ¨¡å¼
            print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(gpu_cmd)}")
            result = subprocess.run(gpu_cmd, capture_output=True, text=True, env=env)
            
            if result.returncode == 0:
                print("âœ… GPUæ¨¡å¼ç‰¹å¾æå–æˆåŠŸ")
                return
            
            print(f"ç›´æ¥GPUæ¨¡å¼å¤±è´¥: {result.stderr}")
            
            # å°è¯•ä½¿ç”¨nvidia-smiæ£€æŸ¥GPUçŠ¶æ€
            try:
                gpu_check = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
                if gpu_check.returncode == 0:
                    print("GPUæ£€æµ‹æ­£å¸¸ï¼Œå°è¯•ä½¿ç”¨xvfb-runåŒ…è£…...")
                    
                    # ä½¿ç”¨xvfb-run + GPU
                    xvfb_gpu_cmd = [
                        'xvfb-run', '-a', '-s', '-screen 0 1024x768x24 -ac +extension GLX +render -noreset'
                    ] + gpu_cmd
                    
                    result = subprocess.run(xvfb_gpu_cmd, capture_output=True, text=True, env=env)
                    
                    if result.returncode == 0:
                        print("âœ… xvfb-run + GPUæ¨¡å¼ç‰¹å¾æå–æˆåŠŸ")
                        return
                    
                    print(f"xvfb-run + GPUæ¨¡å¼ä¹Ÿå¤±è´¥: {result.stderr}")
            except:
                print("æ— æ³•æ£€æŸ¥GPUçŠ¶æ€")
        
        # å¦‚æœGPUæ¨¡å¼å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸ï¼ˆå› ä¸ºç”¨æˆ·è¦æ±‚å¿…é¡»ä½¿ç”¨GPUï¼‰
        if force_gpu:
            raise RuntimeError("å¼ºåˆ¶GPUæ¨¡å¼å¤±è´¥ï¼Œæ— æ³•ç»§ç»­ã€‚è¯·æ£€æŸ¥GPUé©±åŠ¨å’ŒCUDAå®‰è£…ã€‚")
        
        # å¤‡ç”¨CPUæ¨¡å¼ï¼ˆä»…åœ¨ä¸å¼ºåˆ¶GPUæ—¶ä½¿ç”¨ï¼‰
        print("å›é€€åˆ°CPUæ¨¡å¼...")
        env = os.environ.copy()
        env['QT_QPA_PLATFORM'] = 'offscreen'
        env['CUDA_VISIBLE_DEVICES'] = ''
        
        cpu_cmd = [
            'colmap', 'feature_extractor',
            '--database_path', database_path,
            '--image_path', images_dir,
            '--ImageReader.single_camera', '1',
            '--SiftExtraction.use_gpu', '0',
            '--SiftExtraction.max_image_size', str(quality_settings["max_image_size"]),
            '--SiftExtraction.max_num_features', str(quality_settings["max_num_features"])
        ]
        
        result = subprocess.run(cpu_cmd, capture_output=True, text=True, env=env)
        
        if result.returncode != 0:
            raise RuntimeError(f"COLMAPç‰¹å¾æå–å¤±è´¥: {result.stderr}")

    def _run_native_feature_matching(self, database_path: str, matcher_type: str, force_gpu: bool = True):
        """è¿è¡ŒåŸç”ŸCOLMAPç‰¹å¾åŒ¹é…"""
        import subprocess
        import os
        
        # æ„å»ºåŸºç¡€å‘½ä»¤
        if matcher_type == "exhaustive":
            base_cmd = ['colmap', 'exhaustive_matcher', '--database_path', database_path]
        elif matcher_type == "sequential":
            base_cmd = ['colmap', 'sequential_matcher', '--database_path', database_path, '--SequentialMatching.overlap', '10']
        elif matcher_type == "spatial":
            base_cmd = ['colmap', 'spatial_matcher', '--database_path', database_path]
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åŒ¹é…å™¨ç±»å‹: {matcher_type}")
        
        if force_gpu and self.gpu_available:
            print("ğŸš€ ä½¿ç”¨å¼ºåˆ¶GPUæ¨¡å¼è¿è¡ŒCOLMAPç‰¹å¾åŒ¹é…...")
            
            # è®¾ç½®GPUç¯å¢ƒ
            env = self._setup_gpu_environment()
            
            # GPUæ¨¡å¼
            gpu_cmd = base_cmd + [
                '--SiftMatching.use_gpu', '1',
                '--SiftMatching.gpu_index', '0'
            ]
            
            # ç›´æ¥GPUæ¨¡å¼
            result = subprocess.run(gpu_cmd, capture_output=True, text=True, env=env)
            
            if result.returncode == 0:
                print("âœ… GPUæ¨¡å¼ç‰¹å¾åŒ¹é…æˆåŠŸ")
                return
            
            print(f"ç›´æ¥GPUæ¨¡å¼å¤±è´¥: {result.stderr}")
            
            # å°è¯•xvfb-run + GPU
            try:
                xvfb_gpu_cmd = [
                    'xvfb-run', '-a', '-s', '-screen 0 1024x768x24 -ac +extension GLX +render -noreset'
                ] + gpu_cmd
                
                result = subprocess.run(xvfb_gpu_cmd, capture_output=True, text=True, env=env)
                
                if result.returncode == 0:
                    print("âœ… xvfb-run + GPUæ¨¡å¼ç‰¹å¾åŒ¹é…æˆåŠŸ")
                    return
                
                print(f"xvfb-run + GPUæ¨¡å¼ä¹Ÿå¤±è´¥: {result.stderr}")
            except:
                pass
        
        # å¦‚æœGPUæ¨¡å¼å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸ï¼ˆå› ä¸ºç”¨æˆ·è¦æ±‚å¿…é¡»ä½¿ç”¨GPUï¼‰
        if force_gpu:
            raise RuntimeError("å¼ºåˆ¶GPUæ¨¡å¼å¤±è´¥ï¼Œæ— æ³•ç»§ç»­ã€‚è¯·æ£€æŸ¥GPUé©±åŠ¨å’ŒCUDAå®‰è£…ã€‚")
        
        # å¤‡ç”¨CPUæ¨¡å¼
        print("å›é€€åˆ°CPUæ¨¡å¼...")
        env = os.environ.copy()
        env['QT_QPA_PLATFORM'] = 'offscreen'
        env['CUDA_VISIBLE_DEVICES'] = ''
        
        cpu_cmd = base_cmd + ['--SiftMatching.use_gpu', '0']
        result = subprocess.run(cpu_cmd, capture_output=True, text=True, env=env)
        
        if result.returncode != 0:
            raise RuntimeError(f"COLMAPç‰¹å¾åŒ¹é…å¤±è´¥: {result.stderr}")

    def _run_native_mapping(self, database_path: str, images_dir: str, sparse_dir: str, force_gpu: bool = True):
        """è¿è¡ŒåŸç”ŸCOLMAPå¢é‡é‡å»º"""
        import subprocess
        import os
        
        if force_gpu and self.gpu_available:
            print("ğŸš€ ä½¿ç”¨å¼ºåˆ¶GPUæ¨¡å¼è¿è¡ŒCOLMAPé‡å»º...")
            
            # è®¾ç½®GPUç¯å¢ƒ
            env = self._setup_gpu_environment()
            
            # ä½¿ç”¨æ›´å®½æ¾çš„é‡å»ºå‚æ•°
            gpu_cmd = [
                'colmap', 'mapper',
                '--database_path', database_path,
                '--image_path', images_dir,
                '--output_path', sparse_dir,
                '--Mapper.init_min_num_inliers', '5',  # éå¸¸ä½çš„é˜ˆå€¼
                '--Mapper.min_num_matches', '5',       # éå¸¸ä½çš„åŒ¹é…è¦æ±‚
                '--Mapper.max_num_models', '100',      # å…è®¸æ›´å¤šæ¨¡å‹
                '--Mapper.init_min_tri_angle', '1.0',  # é™ä½ä¸‰è§’åŒ–è§’åº¦è¦æ±‚
                '--Mapper.multiple_models', '1',       # å…è®¸å¤šæ¨¡å‹
                '--Mapper.extract_colors', '0'         # å…³é—­é¢œè‰²æå–åŠ é€Ÿ
            ]
            
            print(f"æ‰§è¡ŒGPUé‡å»ºå‘½ä»¤: {' '.join(gpu_cmd)}")
            
            # ç›´æ¥GPUæ¨¡å¼
            result = subprocess.run(gpu_cmd, capture_output=True, text=True, env=env)
            
            print(f"COLMAP mapper è¿”å›ç : {result.returncode}")
            if result.stdout:
                print(f"STDOUT: {result.stdout}")
            if result.stderr:
                print(f"STDERR: {result.stderr}")
            
            if result.returncode == 0:
                print("âœ… GPUæ¨¡å¼é‡å»ºæˆåŠŸ")
                self._check_reconstruction_output(sparse_dir)
                return
            
            print(f"GPUæ¨¡å¼å¤±è´¥: {result.stderr}")
            
            # å¦‚æœæ˜¯å‚æ•°ä¸è¯†åˆ«çš„é”™è¯¯ï¼Œå°è¯•æ›´åŸºç¡€çš„å‚æ•°
            if "unrecognised option" in result.stderr:
                print("æ£€æµ‹åˆ°å‚æ•°ä¸å…¼å®¹ï¼Œå°è¯•åŸºç¡€GPUå‚æ•°...")
                basic_gpu_cmd = [
                    'colmap', 'mapper',
                    '--database_path', database_path,
                    '--image_path', images_dir,
                    '--output_path', sparse_dir
                ]
                
                print(f"æ‰§è¡ŒåŸºç¡€GPUé‡å»ºå‘½ä»¤: {' '.join(basic_gpu_cmd)}")
                result = subprocess.run(basic_gpu_cmd, capture_output=True, text=True, env=env)
                
                print(f"åŸºç¡€GPU mapper è¿”å›ç : {result.returncode}")
                if result.stdout:
                    print(f"STDOUT: {result.stdout}")
                if result.stderr:
                    print(f"STDERR: {result.stderr}")
                
                if result.returncode == 0:
                    print("âœ… GPUæ¨¡å¼åŸºç¡€å‚æ•°é‡å»ºæˆåŠŸ")
                    self._check_reconstruction_output(sparse_dir)
                    return
            
            # å°è¯•xvfb-run + GPU
            try:
                print("å°è¯•xvfb-run + GPUæ¨¡å¼...")
                xvfb_gpu_cmd = [
                    'xvfb-run', '-a', '-s', '-screen 0 1024x768x24 -ac +extension GLX +render -noreset'
                ] + gpu_cmd
                
                result = subprocess.run(xvfb_gpu_cmd, capture_output=True, text=True, env=env)
                
                print(f"xvfb GPU mapper è¿”å›ç : {result.returncode}")
                if result.stdout:
                    print(f"STDOUT: {result.stdout}")
                if result.stderr:
                    print(f"STDERR: {result.stderr}")
                
                if result.returncode == 0:
                    print("âœ… xvfb-run + GPUæ¨¡å¼é‡å»ºæˆåŠŸ")
                    self._check_reconstruction_output(sparse_dir)
                    return
            except Exception as e:
                print(f"xvfb-runæ‰§è¡Œå¤±è´¥: {e}")
        
        # å¦‚æœGPUæ¨¡å¼å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸ï¼ˆå› ä¸ºç”¨æˆ·è¦æ±‚å¿…é¡»ä½¿ç”¨GPUï¼‰
        if force_gpu:
            print("âŒ æ‰€æœ‰GPUæ¨¡å¼å°è¯•éƒ½å¤±è´¥äº†")
            self._check_reconstruction_output(sparse_dir)
            raise RuntimeError("å¼ºåˆ¶GPUæ¨¡å¼å¤±è´¥ï¼Œæ— æ³•ç»§ç»­ã€‚è¯·æ£€æŸ¥å›¾åƒè´¨é‡ã€ç‰¹å¾åŒ¹é…ç»“æœæˆ–è€ƒè™‘ä½¿ç”¨CPUæ¨¡å¼ã€‚")
        
        # å¤‡ç”¨CPUæ¨¡å¼
        print("å›é€€åˆ°CPUæ¨¡å¼...")
        env = os.environ.copy()
        env['QT_QPA_PLATFORM'] = 'offscreen'
        env['CUDA_VISIBLE_DEVICES'] = ''
        
        # CPUæ¨¡å¼ä½¿ç”¨æ›´å®½æ¾çš„å‚æ•°
        cpu_cmd = [
            'colmap', 'mapper',
            '--database_path', database_path,
            '--image_path', images_dir,
            '--output_path', sparse_dir,
            '--Mapper.init_min_num_inliers', '3',  # æä½é˜ˆå€¼
            '--Mapper.min_num_matches', '3',       # æä½åŒ¹é…è¦æ±‚
            '--Mapper.max_num_models', '100',
            '--Mapper.init_min_tri_angle', '1.0',
            '--Mapper.multiple_models', '1',
            '--Mapper.extract_colors', '0'
        ]
        
        print(f"æ‰§è¡ŒCPUé‡å»ºå‘½ä»¤: {' '.join(cpu_cmd)}")
        result = subprocess.run(cpu_cmd, capture_output=True, text=True, env=env)
        
        print(f"CPU mapper è¿”å›ç : {result.returncode}")
        if result.stdout:
            print(f"STDOUT: {result.stdout}")
        if result.stderr:
            print(f"STDERR: {result.stderr}")
        
        if result.returncode != 0:
            print("âŒ CPUæ¨¡å¼é‡å»ºä¹Ÿå¤±è´¥äº†")
            self._check_reconstruction_output(sparse_dir)
            raise RuntimeError(f"COLMAPå¢é‡é‡å»ºå¤±è´¥: {result.stderr}")
        
        print("âœ… CPUæ¨¡å¼é‡å»ºæˆåŠŸ")
        self._check_reconstruction_output(sparse_dir)

    def _check_reconstruction_output(self, sparse_dir: str):
        """æ£€æŸ¥é‡å»ºè¾“å‡ºç»“æœ"""
        print(f"\nğŸ” æ£€æŸ¥é‡å»ºè¾“å‡ºç›®å½•: {sparse_dir}")
        
        if not os.path.exists(sparse_dir):
            print("âŒ é‡å»ºè¾“å‡ºç›®å½•ä¸å­˜åœ¨")
            return
        
        try:
            items = os.listdir(sparse_dir)
            print(f"è¾“å‡ºç›®å½•å†…å®¹: {items}")
            
            if not items:
                print("âš ï¸  é‡å»ºè¾“å‡ºç›®å½•ä¸ºç©º")
                return
            
            for item in items:
                item_path = os.path.join(sparse_dir, item)
                if os.path.isdir(item_path):
                    print(f"\nğŸ“ æ£€æŸ¥å­ç›®å½•: {item}")
                    sub_items = os.listdir(item_path)
                    print(f"  å†…å®¹: {sub_items}")
                    
                    # æ£€æŸ¥COLMAPæ–‡ä»¶
                    colmap_files = ['cameras.txt', 'images.txt', 'points3D.txt']
                    for colmap_file in colmap_files:
                        file_path = os.path.join(item_path, colmap_file)
                        if os.path.exists(file_path):
                            size = os.path.getsize(file_path)
                            print(f"    {colmap_file}: {size} bytes")
                        else:
                            print(f"    {colmap_file}: ä¸å­˜åœ¨")
                            
        except Exception as e:
            print(f"âŒ æ£€æŸ¥è¾“å‡ºç›®å½•å¤±è´¥: {e}")

    def _read_native_reconstruction(self, sparse_dir: str):
        """è¯»å–åŸç”ŸCOLMAPé‡å»ºç»“æœ"""
        # æŸ¥æ‰¾é‡å»ºç›®å½•
        recon_dirs = []
        
        print(f"æ£€æŸ¥ç¨€ç–é‡å»ºç›®å½•: {sparse_dir}")
        
        if not os.path.exists(sparse_dir):
            print(f"é”™è¯¯: ç¨€ç–ç›®å½•ä¸å­˜åœ¨: {sparse_dir}")
            return None
        
        # åˆ—å‡ºæ‰€æœ‰å†…å®¹è¿›è¡Œè¯Šæ–­
        try:
            all_items = os.listdir(sparse_dir)
            print(f"ç¨€ç–ç›®å½•å†…å®¹: {all_items}")
        except Exception as e:
            print(f"æ— æ³•åˆ—å‡ºç¨€ç–ç›®å½•å†…å®¹: {e}")
            return None
        
        for item in all_items:
            item_path = os.path.join(sparse_dir, item)
            print(f"æ£€æŸ¥é¡¹ç›®: {item} -> {item_path}")
            
            if os.path.isdir(item_path):
                print(f"  å‘ç°å­ç›®å½•: {item}")
                
                # æ£€æŸ¥COLMAPé‡å»ºæ–‡ä»¶ï¼ˆæ”¯æŒäºŒè¿›åˆ¶å’Œæ–‡æœ¬æ ¼å¼ï¼‰
                txt_files = ['cameras.txt', 'images.txt', 'points3D.txt']
                bin_files = ['cameras.bin', 'images.bin', 'points3D.bin']
                
                # æ£€æŸ¥æ–‡æœ¬æ ¼å¼æ–‡ä»¶
                txt_exist = all(os.path.exists(os.path.join(item_path, f)) for f in txt_files)
                # æ£€æŸ¥äºŒè¿›åˆ¶æ ¼å¼æ–‡ä»¶
                bin_exist = all(os.path.exists(os.path.join(item_path, f)) for f in bin_files)
                
                print(f"    æ–‡æœ¬æ ¼å¼æ–‡ä»¶: {'å­˜åœ¨' if txt_exist else 'ä¸å­˜åœ¨'}")
                print(f"    äºŒè¿›åˆ¶æ ¼å¼æ–‡ä»¶: {'å­˜åœ¨' if bin_exist else 'ä¸å­˜åœ¨'}")
                
                if txt_exist:
                    # å¦‚æœæ–‡æœ¬æ–‡ä»¶å­˜åœ¨ï¼Œæ£€æŸ¥å…¶æœ‰æ•ˆæ€§
                    if self._validate_reconstruction_dir(item_path, 'txt'):
                        recon_dirs.append(item_path)
                        print(f"    âœ… æœ‰æ•ˆçš„æ–‡æœ¬æ ¼å¼é‡å»ºç›®å½•: {item}")
                    else:
                        print(f"    âš ï¸  æ–‡æœ¬æ–‡ä»¶å­˜åœ¨ä½†å†…å®¹æ— æ•ˆ: {item}")
                        
                elif bin_exist:
                    # å¦‚æœåªæœ‰äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œå°è¯•è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
                    print(f"    ğŸ”„ å‘ç°äºŒè¿›åˆ¶æ ¼å¼ï¼Œå°è¯•è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼...")
                    if self._convert_bin_to_txt(item_path):
                        if self._validate_reconstruction_dir(item_path, 'txt'):
                            recon_dirs.append(item_path)
                            print(f"    âœ… æˆåŠŸè½¬æ¢å¹¶éªŒè¯é‡å»ºç›®å½•: {item}")
                        else:
                            print(f"    âš ï¸  è½¬æ¢æˆåŠŸä½†å†…å®¹æ— æ•ˆ: {item}")
                    else:
                        # è½¬æ¢å¤±è´¥ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨PyColmapè¯»å–äºŒè¿›åˆ¶æ ¼å¼
                        print(f"    ğŸ”„ è½¬æ¢å¤±è´¥ï¼Œå°è¯•ç›´æ¥è¯»å–äºŒè¿›åˆ¶æ ¼å¼...")
                        if self._validate_reconstruction_dir(item_path, 'bin'):
                            recon_dirs.append(item_path)
                            print(f"    âœ… å¯ä»¥ç›´æ¥è¯»å–äºŒè¿›åˆ¶æ ¼å¼: {item}")
                        else:
                            print(f"    âŒ äºŒè¿›åˆ¶æ ¼å¼ä¹Ÿæ— æ³•è¯»å–: {item}")
                else:
                    print(f"    âŒ æ—¢æ²¡æœ‰æ–‡æœ¬æ–‡ä»¶ä¹Ÿæ²¡æœ‰äºŒè¿›åˆ¶æ–‡ä»¶: {item}")
            else:
                print(f"  è·³è¿‡æ–‡ä»¶: {item}")
        
        if not recon_dirs:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„é‡å»ºç›®å½•")
            print("å¯èƒ½çš„åŸå› :")
            print("  1. COLMAPé‡å»ºå¤±è´¥ä½†è¿”å›äº†æˆåŠŸçŠ¶æ€")
            print("  2. é‡å»ºæ–‡ä»¶ç”Ÿæˆä¸å®Œæ•´")
            print("  3. å›¾åƒç‰¹å¾åŒ¹é…ä¸è¶³")
            print("  4. ç›¸æœºå‚æ•°åˆå§‹åŒ–å¤±è´¥")
            return None
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªé‡å»ºç›®å½•
        recon_dir = recon_dirs[0]
        print(f"âœ… é€‰æ‹©é‡å»ºç›®å½•: {recon_dir}")
        
        # ä½¿ç”¨PyColmapè¯»å–COLMAPæ ¼å¼æ–‡ä»¶
        try:
            print("æ­£åœ¨ä½¿ç”¨PyColmapè¯»å–é‡å»ºæ•°æ®...")
            reconstruction = self.pycolmap.Reconstruction(recon_dir)
            
            # éªŒè¯è¯»å–çš„æ•°æ®
            num_cameras = len(reconstruction.cameras)
            num_images = len(reconstruction.images)
            num_points = len(reconstruction.points3D)
            
            print(f"âœ… æˆåŠŸè¯»å–é‡å»ºï¼š{num_cameras}ä¸ªç›¸æœºï¼Œ{num_images}å¼ å›¾åƒï¼Œ{num_points}ä¸ª3Dç‚¹")
            
            if num_cameras == 0:
                print("âš ï¸  è­¦å‘Š: æ²¡æœ‰ç›¸æœºæ•°æ®")
                return None
            
            if num_images == 0:
                print("âš ï¸  è­¦å‘Š: æ²¡æœ‰æ³¨å†Œçš„å›¾åƒ")
                return None
            
            return reconstruction
            
        except Exception as e:
            print(f"âŒ PyColmapè¯»å–å¤±è´¥: {e}")
            print("å°è¯•æ‰‹åŠ¨æ£€æŸ¥æ–‡ä»¶æ ¼å¼...")
            
            # å°è¯•æ‰‹åŠ¨è¯»å–å’Œè¯Šæ–­æ–‡ä»¶
            self._diagnose_colmap_files(recon_dir)
            return None

    def _convert_bin_to_txt(self, recon_dir: str) -> bool:
        """å°†COLMAPäºŒè¿›åˆ¶æ ¼å¼è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼"""
        import subprocess
        
        try:
            print("    æ‰§è¡ŒCOLMAPæ¨¡å‹è½¬æ¢...")
            
            # è®¾ç½®ç¯å¢ƒå˜é‡
            env = os.environ.copy()
            env['QT_QPA_PLATFORM'] = 'offscreen'
            
            # ä½¿ç”¨COLMAPçš„model_converterå‘½ä»¤
            cmd = [
                'colmap', 'model_converter',
                '--input_path', recon_dir,
                '--output_path', recon_dir,
                '--output_type', 'TXT'
            ]
            
            print(f"    è½¬æ¢å‘½ä»¤: {' '.join(cmd)}")
            result = subprocess.run(cmd, capture_output=True, text=True, env=env)
            
            print(f"    è½¬æ¢è¿”å›ç : {result.returncode}")
            if result.stdout:
                print(f"    STDOUT: {result.stdout}")
            if result.stderr:
                print(f"    STDERR: {result.stderr}")
            
            if result.returncode == 0:
                print("    âœ… äºŒè¿›åˆ¶åˆ°æ–‡æœ¬è½¬æ¢æˆåŠŸ")
                return True
            else:
                print(f"    âŒ è½¬æ¢å¤±è´¥: {result.stderr}")
                return False
                
        except Exception as e:
            print(f"    âŒ è½¬æ¢è¿‡ç¨‹å‡ºé”™: {e}")
            return False

    def _validate_reconstruction_dir(self, recon_dir: str, format_type: str) -> bool:
        """éªŒè¯é‡å»ºç›®å½•æ˜¯å¦æœ‰æ•ˆ"""
        try:
            if format_type == 'txt':
                # éªŒè¯æ–‡æœ¬æ ¼å¼
                required_files = ['cameras.txt', 'images.txt', 'points3D.txt']
                for req_file in required_files:
                    file_path = os.path.join(recon_dir, req_file)
                    if not os.path.exists(file_path):
                        return False
                    if not self._validate_colmap_file(file_path, req_file):
                        return False
                return True
                
            elif format_type == 'bin':
                # éªŒè¯äºŒè¿›åˆ¶æ ¼å¼ - å°è¯•ç”¨PyColmapè¯»å–
                try:
                    test_reconstruction = self.pycolmap.Reconstruction(recon_dir)
                    return len(test_reconstruction.cameras) > 0 and len(test_reconstruction.images) > 0
                except:
                    return False
                    
        except Exception as e:
            print(f"      éªŒè¯ç›®å½•å¤±è´¥: {e}")
            return False

    def _validate_colmap_file(self, file_path: str, file_type: str) -> bool:
        """éªŒè¯COLMAPæ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ"""
        try:
            with open(file_path, 'r') as f:
                lines = f.readlines()
            
            # è¿‡æ»¤æ‰æ³¨é‡Šå’Œç©ºè¡Œ
            data_lines = [line.strip() for line in lines if line.strip() and not line.strip().startswith('#')]
            
            if file_type == 'cameras.txt':
                # ç›¸æœºæ–‡ä»¶åº”è¯¥è‡³å°‘æœ‰ä¸€è¡Œæ•°æ®
                if len(data_lines) == 0:
                    print(f"      {file_type}: æ²¡æœ‰ç›¸æœºæ•°æ®")
                    return False
                print(f"      {file_type}: {len(data_lines)} ä¸ªç›¸æœº")
                
            elif file_type == 'images.txt':
                # å›¾åƒæ–‡ä»¶åº”è¯¥æœ‰æˆå¯¹çš„è¡Œï¼ˆå›¾åƒè¡Œ + ç‰¹å¾ç‚¹è¡Œï¼‰
                if len(data_lines) == 0:
                    print(f"      {file_type}: æ²¡æœ‰å›¾åƒæ•°æ®")
                    return False
                print(f"      {file_type}: {len(data_lines)} è¡Œæ•°æ®")
                
            elif file_type == 'points3D.txt':
                # 3Dç‚¹æ–‡ä»¶å¯ä»¥ä¸ºç©ºï¼ˆæ²¡æœ‰3Dç‚¹ä¹Ÿèƒ½è¿›è¡Œç›¸æœºä¼°è®¡ï¼‰
                print(f"      {file_type}: {len(data_lines)} ä¸ª3Dç‚¹")
            
            return True
            
        except Exception as e:
            print(f"      {file_type}: è¯»å–å¤±è´¥ - {e}")
            return False

    def _diagnose_colmap_files(self, recon_dir: str):
        """è¯Šæ–­COLMAPæ–‡ä»¶å†…å®¹"""
        print("\nğŸ” è¯¦ç»†æ–‡ä»¶è¯Šæ–­:")
        
        files_to_check = ['cameras.txt', 'images.txt', 'points3D.txt']
        
        for filename in files_to_check:
            file_path = os.path.join(recon_dir, filename)
            print(f"\nğŸ“„ æ£€æŸ¥ {filename}:")
            
            try:
                with open(file_path, 'r') as f:
                    lines = f.readlines()
                
                print(f"  æ€»è¡Œæ•°: {len(lines)}")
                
                # æ˜¾ç¤ºå‰å‡ è¡Œå†…å®¹
                print("  å‰5è¡Œå†…å®¹:")
                for i, line in enumerate(lines[:5]):
                    print(f"    {i+1}: {repr(line.strip())}")
                
                # ç»Ÿè®¡æ•°æ®è¡Œ
                data_lines = [line for line in lines if line.strip() and not line.strip().startswith('#')]
                print(f"  æ•°æ®è¡Œæ•°: {len(data_lines)}")
                
                if len(data_lines) > 0:
                    print("  é¦–ä¸ªæ•°æ®è¡Œ:")
                    print(f"    {repr(data_lines[0].strip())}")
                
            except Exception as e:
                print(f"  âŒ æ— æ³•è¯»å–æ–‡ä»¶: {e}")

    def _parse_native_reconstruction(self, reconstruction, num_input_images: int) -> Dict:
        """è§£æåŸç”ŸCOLMAPé‡å»ºç»“æœ"""
        try:
            # è§£æç›¸æœºå†…å‚
            intrinsics = self._parse_camera_intrinsics(reconstruction)
            
            # è§£æä½å§¿
            poses = self._parse_camera_poses(reconstruction)
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            statistics = self._calculate_statistics(poses, reconstruction)
            
            # ç‚¹äº‘ä¿¡æ¯
            point_cloud_info = {
                "num_points": len(reconstruction.points3D),
                "num_cameras": len(reconstruction.cameras),
                "num_registered_images": len(reconstruction.images),
                "num_input_images": num_input_images,
                "registration_ratio": len(reconstruction.images) / max(num_input_images, 1)
            }
            
            return {
                "success": True,
                "intrinsics": intrinsics,
                "poses": poses,
                "statistics": statistics,
                "frame_count": len(poses),
                "point_cloud": point_cloud_info
            }
            
        except Exception as e:
            print(f"è§£æåŸç”Ÿé‡å»ºç»“æœå¤±è´¥: {e}")
            return {
                "success": False,
                "error": f"è§£æå¤±è´¥: {str(e)}",
                "intrinsics": None,
                "poses": [],
                "statistics": {},
                "frame_count": 0,
                "point_cloud": {}
            }

    def _parse_camera_intrinsics(self, reconstruction) -> Dict:
        """è§£æç›¸æœºå†…å‚"""
        if len(reconstruction.cameras) == 0:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°ç›¸æœºå‚æ•°")
        
        # è·å–ç¬¬ä¸€ä¸ªç›¸æœºçš„å‚æ•°ï¼ˆå‡è®¾æ‰€æœ‰å›¾åƒä½¿ç”¨åŒä¸€ç›¸æœºï¼‰
        camera = list(reconstruction.cameras.values())[0]
        
        # è·å–åŸºæœ¬å‚æ•°
        focal_length = float(camera.params[0]) if len(camera.params) > 0 else 800.0
        focal_length_y = float(camera.params[1]) if len(camera.params) > 1 else focal_length
        
        # è·å–ä¸»ç‚¹
        if len(camera.params) > 3:
            principal_point = [float(camera.params[2]), float(camera.params[3])]
        else:
            principal_point = [camera.width / 2, camera.height / 2]
        
        # è·å–ç•¸å˜å‚æ•°
        distortion = [float(p) for p in camera.params[4:]] if len(camera.params) > 4 else []
        
        # è·å–ç›¸æœºæ¨¡å‹åç§°
        camera_model = "PINHOLE"  # é»˜è®¤å€¼
        if hasattr(camera, 'model_name'):
            camera_model = camera.model_name
        elif hasattr(camera, 'model_id'):
            model_names = {
                0: "SIMPLE_PINHOLE", 1: "PINHOLE", 2: "SIMPLE_RADIAL",
                3: "RADIAL", 4: "OPENCV", 5: "OPENCV_FISHEYE"
            }
            camera_model = model_names.get(camera.model_id, f"MODEL_{camera.model_id}")
        
        return {
            "focal_length": focal_length,
            "focal_length_y": focal_length_y,
            "principal_point": principal_point,
            "image_size": [camera.width, camera.height],
            "camera_model": camera_model,
            "distortion": distortion
        }

    def _parse_camera_poses(self, reconstruction) -> List[Dict]:
        """è§£æç›¸æœºä½å§¿"""
        poses = []
        
        for image_id, image in reconstruction.images.items():
            # è·å–ä½å§¿æ•°æ® (å…¼å®¹ä¸åŒç‰ˆæœ¬çš„ pycolmap)
            quat = None
            trans = None
            
            # é¦–æ¬¡è¿­ä»£æ—¶æ‰“å°å¯ç”¨å±æ€§ä»¥ä¾¿è°ƒè¯•
            if image_id == list(reconstruction.images.keys())[0]:
                print(f"PyColmap Image å¯¹è±¡å¯ç”¨å±æ€§: {[attr for attr in dir(image) if not attr.startswith('_')]}")
            
            # å°è¯•å¤šç§å¯èƒ½çš„API
            if hasattr(image, "qvec") and hasattr(image, "tvec"):
                # æ—§ç‰ˆ API: ç›´æ¥è®¿é—®å±æ€§
                quat = image.qvec.tolist() if hasattr(image.qvec, 'tolist') else list(image.qvec)
                trans = image.tvec.tolist() if hasattr(image.tvec, 'tolist') else list(image.tvec)
            elif hasattr(image, "cam_from_world"):
                # æ–°ç‰ˆ API: ä½¿ç”¨ cam_from_world å±æ€§
                # cam_from_world æ˜¯ä¸€ä¸ª Rigid3d å¯¹è±¡ï¼ŒåŒ…å«æ—‹è½¬å’Œå¹³ç§»
                cam_from_world = image.cam_from_world
                if hasattr(cam_from_world, "rotation"):
                    # è·å–å››å…ƒæ•°
                    rotation = cam_from_world.rotation
                    if hasattr(rotation, "quat"):
                        quat = rotation.quat.tolist()
                    elif hasattr(rotation, "quaternion"):
                        quat = rotation.quaternion.tolist()
                    else:
                        # å°è¯•ä»æ—‹è½¬çŸ©é˜µè½¬æ¢
                        print(f"è­¦å‘Š: æ— æ³•ç›´æ¥è·å–å››å…ƒæ•°ï¼Œå°è¯•å…¶ä»–æ–¹æ³•")
                
                if hasattr(cam_from_world, "translation"):
                    trans = cam_from_world.translation.tolist()
            elif hasattr(image, "projection_center"):
                # å¦ä¸€ç§å¯èƒ½: ä½¿ç”¨ projection_center ä½œä¸ºä½ç½®
                print(f"ä½¿ç”¨ projection_center ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ")
                trans = image.projection_center().tolist()
                # å°è¯•è·å–æ—‹è½¬
                if hasattr(image, "rotation_matrix"):
                    # ä»æ—‹è½¬çŸ©é˜µè®¡ç®—å››å…ƒæ•°
                    print(f"ä» rotation_matrix è®¡ç®—å››å…ƒæ•°")
            
            # å¦‚æœä»ç„¶æ— æ³•è·å–æ•°æ®ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
            if quat is None or trans is None:
                print(f"è­¦å‘Š: æ— æ³•è·å–å›¾åƒ {image_id} çš„å®Œæ•´ä½å§¿æ•°æ®")
                # æä¾›é»˜è®¤å€¼ä»¥é¿å…å´©æºƒ
                if quat is None:
                    quat = [1.0, 0.0, 0.0, 0.0]  # å•ä½å››å…ƒæ•°
                if trans is None:
                    trans = [0.0, 0.0, 0.0]  # åŸç‚¹
            
            # è½¬æ¢ä¸ºæ¬§æ‹‰è§’
            euler = self._quaternion_to_euler(quat)
            
            # è·å–å›¾åƒåç§°
            image_name = image.name if hasattr(image, 'name') else f"image_{image_id}"
            
            pose = {
                "position": trans,
                "rotation_quaternion": quat,
                "rotation_euler": euler,
                "image_name": image_name,
                "image_id": image_id
            }
            poses.append(pose)
        
        # æŒ‰å›¾åƒåç§°æ’åºï¼Œç¡®ä¿é¡ºåº
        poses.sort(key=lambda x: x["image_name"])
        
        return poses

    def _quaternion_to_euler(self, quat: List[float]) -> List[float]:
        """å››å…ƒæ•°è½¬æ¬§æ‹‰è§’"""
        qw, qx, qy, qz = quat
        
        # Roll (x-axis rotation)
        sinr_cosp = 2 * (qw * qx + qy * qz)
        cosr_cosp = 1 - 2 * (qx * qx + qy * qy)
        roll = np.arctan2(sinr_cosp, cosr_cosp)
        
        # Pitch (y-axis rotation)
        sinp = 2 * (qw * qy - qz * qx)
        if abs(sinp) >= 1:
            pitch = np.copysign(np.pi / 2, sinp)
        else:
            pitch = np.arcsin(sinp)
        
        # Yaw (z-axis rotation)
        siny_cosp = 2 * (qw * qz + qx * qy)
        cosy_cosp = 1 - 2 * (qy * qy + qz * qz)
        yaw = np.arctan2(siny_cosp, cosy_cosp)
        
        return [roll, pitch, yaw]

    def _calculate_statistics(self, poses: List[Dict], reconstruction) -> Dict:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        if len(poses) < 2:
            return {
                "total_distance": 0.0,
                "average_speed": 0.0,
                "num_poses": len(poses),
                "num_3d_points": len(reconstruction.points3D)
            }
        
        # è®¡ç®—è½¨è¿¹æ€»é•¿åº¦
        total_distance = 0.0
        for i in range(1, len(poses)):
            pos1 = np.array(poses[i-1]["position"])
            pos2 = np.array(poses[i]["position"])
            distance = np.linalg.norm(pos2 - pos1)
            total_distance += distance
        
        average_speed = total_distance / (len(poses) - 1) if len(poses) > 1 else 0.0
        
        return {
            "total_distance": float(total_distance),
            "average_speed": float(average_speed),
            "num_poses": len(poses),
            "num_3d_points": len(reconstruction.points3D),
            "mean_track_length": np.mean([len(point.track.elements) for point in reconstruction.points3D.values()]) if len(reconstruction.points3D) > 0 else 0.0
        }


class ImageSequenceCameraEstimator:
    """å›¾ç‰‡åºåˆ—ç›¸æœºå‚æ•°ä¼°è®¡èŠ‚ç‚¹"""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "images": ("IMAGE", {
                    "tooltip": "è¾“å…¥å›¾ç‰‡åºåˆ—"
                }),
            },
            "optional": {
                "colmap_feature_type": (["sift", "superpoint", "disk"], {
                    "default": "sift",
                    "tooltip": "COLMAPç‰¹å¾æ£€æµ‹å™¨ç±»å‹ï¼ŒSIFTæœ€ç¨³å®š"
                }),
                "colmap_matcher_type": (["exhaustive", "sequential", "spatial"], {
                    "default": "sequential",
                    "tooltip": "COLMAPåŒ¹é…ç­–ç•¥ï¼Œsequentialé€‚åˆæœ‰åºåºåˆ—"
                }),
                "colmap_quality": (["low", "medium", "high", "extreme"], {
                    "default": "medium",
                    "tooltip": "COLMAPé‡å»ºè´¨é‡ï¼Œhigherè´¨é‡éœ€è¦æ›´å¤šæ—¶é—´"
                }),
                "enable_dense_reconstruction": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "æ˜¯å¦å¯ç”¨å¯†é›†é‡å»ºï¼ˆéœ€è¦æ›´å¤šè®¡ç®—èµ„æºå’ŒCUDAæ”¯æŒï¼‰"
                }),
                "force_gpu": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¼ºåˆ¶ä½¿ç”¨GPUæ¨¡å¼ï¼ˆå¤±è´¥æ—¶ä¸å›é€€åˆ°CPUï¼‰"
                }),
                "enable_visualization": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "æ˜¯å¦ç”Ÿæˆè½¨è¿¹å¯è§†åŒ–å›¾åƒ"
                }),
                "output_format": (["json", "detailed_json"], {
                    "default": "detailed_json",
                    "tooltip": "è¾“å‡ºæ ¼å¼ï¼Œè¯¦ç»†æ¨¡å¼åŒ…å«æ›´å¤šç»Ÿè®¡ä¿¡æ¯"
                })
            }
        }

    RETURN_TYPES = ("STRING", "IMAGE", "STRING", "STRING", "STRING")
    RETURN_NAMES = ("intrinsics_json", "trajectory_visualization", "poses_json", "statistics_json", "point_cloud_info")
    FUNCTION = "estimate_camera_parameters"
    CATEGORY = "ğŸ’ƒVVL/VideoCamera"

    def __init__(self):
        try:
            self.estimator = ColmapCameraEstimator()
        except ImportError as e:
            print(f"COLMAPåˆå§‹åŒ–å¤±è´¥: {e}")
            self.estimator = None

    def estimate_camera_parameters(self, images, 
                                 colmap_feature_type: str = "sift",
                                 colmap_matcher_type: str = "sequential", 
                                 colmap_quality: str = "medium",
                                 enable_dense_reconstruction: bool = False,
                                 force_gpu: bool = True,
                                 enable_visualization: bool = True,
                                 output_format: str = "detailed_json") -> tuple:
        """ä»å›¾ç‰‡åºåˆ—ä¼°è®¡ç›¸æœºå‚æ•°çš„ä¸»å‡½æ•°"""
        
        try:
            if self.estimator is None:
                raise RuntimeError("COLMAPåˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥PyColmapå®‰è£…")
            
            # æ£€æŸ¥è¾“å…¥
            if images is None:
                raise ValueError("æœªæä¾›å›¾ç‰‡è¾“å…¥")
            
            # è½¬æ¢è¾“å…¥æ ¼å¼
            if isinstance(images, torch.Tensor):
                if images.dim() == 4:  # Batch of images
                    image_list = [images[i] for i in range(images.shape[0])]
                else:  # Single image
                    image_list = [images]
            elif isinstance(images, list):
                image_list = images
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å›¾ç‰‡è¾“å…¥æ ¼å¼: {type(images)}")
            
            print(f"å¼€å§‹å¤„ç† {len(image_list)} å¼ å›¾ç‰‡")
            print(f"GPUæ¨¡å¼: {'å¼ºåˆ¶å¯ç”¨' if force_gpu else 'è‡ªé€‚åº”'}")
            
            # ä½¿ç”¨COLMAPè¿›è¡Œä¼°è®¡
            result = self.estimator.estimate_from_images(
                images=image_list,
                colmap_feature_type=colmap_feature_type,
                colmap_matcher_type=colmap_matcher_type,
                colmap_quality=colmap_quality,
                enable_dense_reconstruction=enable_dense_reconstruction,
                force_gpu=force_gpu
            )
            
            if not result["success"]:
                raise RuntimeError(f"ç›¸æœºå‚æ•°ä¼°è®¡å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
            # å‡†å¤‡è¾“å‡º
            intrinsics_json = self._format_intrinsics_output(result["intrinsics"], output_format)
            poses_json = self._format_poses_output(result["poses"], output_format)
            statistics_json = self._format_statistics_output(result["statistics"], result, output_format)
            point_cloud_info = self._format_point_cloud_output(result["point_cloud"], output_format)
            
            # å¤„ç†å¯è§†åŒ–å›¾åƒ
            if enable_visualization and result["poses"]:
                trajectory_img = self._create_trajectory_visualization(result["poses"])
            else:
                trajectory_img = self._create_empty_visualization()
            
            print(f"æˆåŠŸå¤„ç† {result['frame_count']} å¼ å›¾ç‰‡")
            if result['intrinsics']:
                print(f"ä¼°è®¡çš„ç„¦è·: {result['intrinsics']['focal_length']:.2f}")
            
            return (intrinsics_json, trajectory_img, poses_json, statistics_json, point_cloud_info)
            
        except Exception as e:
            error_msg = f"å›¾ç‰‡åºåˆ—ç›¸æœºå‚æ•°ä¼°è®¡å‡ºé”™: {str(e)}"
            print(error_msg)
            
            # è¿”å›é”™è¯¯ä¿¡æ¯
            error_json = json.dumps({"error": error_msg, "success": False}, ensure_ascii=False, indent=2)
            empty_img = self._create_empty_visualization()
            
            return (error_json, empty_img, error_json, error_json, error_json)

    def _format_intrinsics_output(self, intrinsics: Dict, output_format: str) -> str:
        """æ ¼å¼åŒ–å†…å‚è¾“å‡º"""
        if output_format == "json":
            simplified = {
                "focal_length": intrinsics["focal_length"],
                "principal_point": intrinsics["principal_point"],
                "image_size": intrinsics["image_size"]
            }
            return json.dumps(simplified, ensure_ascii=False, indent=2)
        else:
            return json.dumps(intrinsics, ensure_ascii=False, indent=2)

    def _format_poses_output(self, poses: List[Dict], output_format: str) -> str:
        """æ ¼å¼åŒ–ä½å§¿è¾“å‡º"""
        if output_format == "json":
            simplified_poses = []
            for i, pose in enumerate(poses):
                simplified_poses.append({
                    "frame": i,
                    "position": pose["position"],
                    "rotation_euler": pose["rotation_euler"]
                })
            return json.dumps(simplified_poses, ensure_ascii=False, indent=2)
        else:
            detailed_poses = []
            for i, pose in enumerate(poses):
                detailed_pose = pose.copy()
                detailed_pose["frame"] = i
                detailed_poses.append(detailed_pose)
            return json.dumps(detailed_poses, ensure_ascii=False, indent=2)

    def _format_statistics_output(self, statistics: Dict, result: Dict, output_format: str) -> str:
        """æ ¼å¼åŒ–ç»Ÿè®¡ä¿¡æ¯è¾“å‡º"""
        if output_format == "json":
            simplified_stats = {
                "frame_count": result["frame_count"],
                "total_distance": statistics.get("total_distance", 0),
                "num_3d_points": statistics.get("num_3d_points", 0),
                "success": result["success"]
            }
            return json.dumps(simplified_stats, ensure_ascii=False, indent=2)
        else:
            detailed_stats = statistics.copy()
            detailed_stats.update({
                "frame_count": result["frame_count"],
                "success": result["success"]
            })
            return json.dumps(detailed_stats, ensure_ascii=False, indent=2)

    def _format_point_cloud_output(self, point_cloud_info: Dict, output_format: str) -> str:
        """æ ¼å¼åŒ–ç‚¹äº‘ä¿¡æ¯è¾“å‡º"""
        if output_format == "json":
            simplified = {
                "num_points": point_cloud_info.get("num_points", 0),
                "registration_ratio": point_cloud_info.get("registration_ratio", 0)
            }
            return json.dumps(simplified, ensure_ascii=False, indent=2)
        else:
            return json.dumps(point_cloud_info, ensure_ascii=False, indent=2)

    def _create_trajectory_visualization(self, poses: List[Dict]) -> torch.Tensor:
        """åˆ›å»ºè½¨è¿¹å¯è§†åŒ–"""
        try:
            import matplotlib.pyplot as plt
            from mpl_toolkits.mplot3d import Axes3D
            
            # æå–ä½ç½®ä¿¡æ¯
            positions = np.array([pose["position"] for pose in poses])
            
            fig = plt.figure(figsize=(12, 9))
            ax = fig.add_subplot(111, projection='3d')
            
            # ç»˜åˆ¶è½¨è¿¹çº¿
            if len(positions) > 1:
                ax.plot(positions[:, 0], positions[:, 1], positions[:, 2], 
                       'b-', linewidth=3, alpha=0.8, label='Camera Path')
            
            # ç”¨é¢œè‰²æ¸å˜è¡¨ç¤ºæ—¶é—´è¿›ç¨‹
            colors = plt.cm.viridis(np.linspace(0, 1, len(positions)))
            scatter = ax.scatter(positions[:, 0], positions[:, 1], positions[:, 2], 
                               c=colors, s=60, alpha=0.9, edgecolors='black', linewidth=0.5)
            
            # æ ‡è®°èµ·ç‚¹å’Œç»ˆç‚¹
            if len(positions) > 0:
                ax.scatter([positions[0, 0]], [positions[0, 1]], [positions[0, 2]], 
                          c='green', s=150, marker='^', label='Start', edgecolors='darkgreen', linewidth=2)
            if len(positions) > 1:
                ax.scatter([positions[-1, 0]], [positions[-1, 1]], [positions[-1, 2]], 
                          c='red', s=150, marker='o', label='End', edgecolors='darkred', linewidth=2)
            
            # è®¾ç½®åæ ‡è½´
            ax.set_xlabel('X (meters)', fontsize=12)
            ax.set_ylabel('Y (meters)', fontsize=12)
            ax.set_zlabel('Z (meters)', fontsize=12)
            ax.set_title('COLMAP Camera Trajectory (3D View)', fontsize=14, fontweight='bold')
            
            # æ·»åŠ å›¾ä¾‹å’Œé¢œè‰²æ¡
            ax.legend(loc='upper right', fontsize=10)
            cbar = plt.colorbar(scatter, ax=ax, shrink=0.6, aspect=20)
            cbar.set_label('Time Progress', fontsize=10)
            
            # è®¾ç½®ç›¸ç­‰çš„åæ ‡è½´æ¯”ä¾‹
            max_range = np.array([positions.max(axis=0) - positions.min(axis=0)]).max() / 2.0
            mid_x = (positions.max(axis=0)[0] + positions.min(axis=0)[0]) * 0.5
            mid_y = (positions.max(axis=0)[1] + positions.min(axis=0)[1]) * 0.5
            mid_z = (positions.max(axis=0)[2] + positions.min(axis=0)[2]) * 0.5
            
            ax.set_xlim(mid_x - max_range, mid_x + max_range)
            ax.set_ylim(mid_y - max_range, mid_y + max_range)
            ax.set_zlim(mid_z - max_range, mid_z + max_range)
            
            ax.grid(True, alpha=0.3)
            ax.view_init(elev=20, azim=45)
            
            # ä¿å­˜ä¸ºå›¾åƒ
            with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
                plt.savefig(tmp.name, dpi=120, bbox_inches='tight', facecolor='white')
                plt.close()
                
                # è¯»å–å›¾åƒ
                img = cv2.imread(tmp.name)
                os.unlink(tmp.name)
                
                if img is not None:
                    # BGRè½¬RGB
                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                    img_tensor = torch.from_numpy(img.astype(np.float32) / 255.0)
                    return img_tensor.unsqueeze(0)
                else:
                    return self._create_empty_visualization()
        
        except Exception as e:
            print(f"åˆ›å»ºå¯è§†åŒ–å¤±è´¥: {e}")
            return self._create_empty_visualization()

    def _create_empty_visualization(self) -> torch.Tensor:
        """åˆ›å»ºç©ºçš„å¯è§†åŒ–å›¾åƒ"""
        img = np.ones((400, 600, 3), dtype=np.float32) * 0.1
        
        try:
            img_uint8 = (img * 255).astype(np.uint8)
            cv2.putText(img_uint8, "No Trajectory", (180, 180), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 2)
            cv2.putText(img_uint8, "Visualization", (170, 220), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 2)
            img = img_uint8.astype(np.float32) / 255.0
        except:
            pass
        
        return torch.from_numpy(img).unsqueeze(0)

# èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "ImageSequenceCameraEstimator": ImageSequenceCameraEstimator
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "ImageSequenceCameraEstimator": "VVL Image Sequence Camera Estimator"
} 