import torch
import numpy as np
import json
import os
import tempfile
import cv2
from PIL import Image
from typing import List, Dict, Any

# æ·»åŠ ComfyUIç±»å‹å¯¼å…¥
try:
    from comfy.comfy_types import IO
except ImportError:
    # å¦‚æœæ— æ³•å¯¼å…¥ï¼Œåˆ›å»ºä¸€ä¸ªå…¼å®¹çš„ç±»
    class IO:
        VIDEO = "VIDEO"

# ä¿®å¤å¯¼å…¥è·¯å¾„
try:
    from .utils.camera_utils import CameraParameterEstimator
except ImportError:
    try:
        # å°è¯•ç›¸å¯¹å¯¼å…¥
        import sys
        current_dir = os.path.dirname(os.path.abspath(__file__))
        utils_path = os.path.join(current_dir, 'utils')
        if utils_path not in sys.path:
            sys.path.insert(0, utils_path)
        from camera_utils import CameraParameterEstimator
    except ImportError:
        # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ
        import sys
        current_dir = os.path.dirname(os.path.abspath(__file__))
        sys.path.insert(0, current_dir)
        try:
            from utils.camera_utils import CameraParameterEstimator
        except ImportError as e:
            print(f"æ— æ³•å¯¼å…¥ CameraParameterEstimator: {e}")
            # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿç±»ï¼Œé¿å…å¯åŠ¨å¤±è´¥
            class CameraParameterEstimator:
                def __init__(self):
                    self.error = "CameraParameterEstimator å¯¼å…¥å¤±è´¥"
                
                def estimate_from_video(self, *args, **kwargs):
                    return {
                        "success": False,
                        "error": self.error,
                        "intrinsics": None,
                        "poses": [],
                        "statistics": None,
                        "frame_count": 0,
                        "trajectory_visualization": None
                    }

# å…¨å±€ä¼°è®¡å™¨å®ä¾‹
_ESTIMATOR = None

class VideoCameraEstimator:
    """è§†é¢‘ç›¸æœºå‚æ•°ä¼°è®¡èŠ‚ç‚¹"""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "video": (IO.VIDEO, {
                    "tooltip": "ä»LoadVideoèŠ‚ç‚¹è¾“å…¥çš„è§†é¢‘ï¼Œæˆ–è€…ç›´æ¥è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„"
                }),
            },
            "optional": {
                "video_path": ("STRING", {
                    "default": "",
                    "tooltip": "è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œå¦‚æœvideoè¾“å…¥ä¸ºç©ºåˆ™ä½¿ç”¨æ­¤è·¯å¾„ï¼‰"
                }),
                "frame_interval": ("INT", {
                    "default": 10,
                    "min": 1,
                    "max": 100,
                    "step": 1,
                    "tooltip": "æå–å¸§çš„é—´éš”ï¼Œæ•°å€¼è¶Šå°æå–çš„å¸§è¶Šå¤š"
                }),
                "max_frames": ("INT", {
                    "default": 50,
                    "min": 5,
                    "max": 200,
                    "step": 5,
                    "tooltip": "æœ€å¤§æå–å¸§æ•°ï¼Œç”¨äºæ§åˆ¶è®¡ç®—é‡"
                }),
                "estimation_method": (["opencv_sfm", "feature_matching", "hybrid"], {
                    "default": "hybrid",
                    "tooltip": "ç›¸æœºå‚æ•°ä¼°è®¡æ–¹æ³•"
                }),
                "enable_visualization": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "æ˜¯å¦ç”Ÿæˆè½¨è¿¹å¯è§†åŒ–å›¾åƒ"
                }),
                "output_format": (["json", "detailed_json"], {
                    "default": "detailed_json",
                    "tooltip": "è¾“å‡ºæ ¼å¼ï¼Œè¯¦ç»†æ¨¡å¼åŒ…å«æ›´å¤šç»Ÿè®¡ä¿¡æ¯"
                })
            }
        }

    RETURN_TYPES = ("STRING", "IMAGE", "STRING", "STRING")
    RETURN_NAMES = ("intrinsics_json", "trajectory_visualization", "poses_json", "statistics_json")
    FUNCTION = "estimate_camera_parameters"
    CATEGORY = "ğŸ’ƒVVL/VideoCamera"

    def __init__(self):
        global _ESTIMATOR
        if _ESTIMATOR is None:
            _ESTIMATOR = CameraParameterEstimator()
        self.estimator = _ESTIMATOR

    def estimate_camera_parameters(self, video, video_path: str = "", frame_interval: int = 10, max_frames: int = 50,
                                 estimation_method: str = "hybrid", enable_visualization: bool = True,
                                 output_format: str = "detailed_json") -> tuple:
        """
        ä»è§†é¢‘ä¼°è®¡ç›¸æœºå‚æ•°çš„ä¸»å‡½æ•°
        
        Args:
            video: æ¥è‡ªLoadVideoèŠ‚ç‚¹çš„è§†é¢‘å¯¹è±¡æˆ–None
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆå¤‡ç”¨ï¼‰
            frame_interval: å¸§æå–é—´éš”
            max_frames: æœ€å¤§å¸§æ•°
            estimation_method: ä¼°è®¡æ–¹æ³•
            enable_visualization: æ˜¯å¦ç”Ÿæˆå¯è§†åŒ–
            output_format: è¾“å‡ºæ ¼å¼
            
        Returns:
            tuple: (å†…å‚JSON, è½¨è¿¹å¯è§†åŒ–å›¾åƒ, ä½å§¿JSON, ç»Ÿè®¡ä¿¡æ¯JSON)
        """
        
        try:
            # ç¡®å®šè§†é¢‘æ–‡ä»¶è·¯å¾„
            actual_video_path = None
            
            # ä¼˜å…ˆä½¿ç”¨videoè¾“å…¥ï¼ˆæ¥è‡ªLoadVideoèŠ‚ç‚¹ï¼‰
            if video is not None:
                # å¦‚æœvideoæ˜¯VideoFromFileå¯¹è±¡ï¼Œè·å–å…¶æ–‡ä»¶è·¯å¾„
                if hasattr(video, '_VideoFromFile__file'):
                    # è®¿é—®ç§æœ‰å±æ€§ __file
                    file_attr = video._VideoFromFile__file
                    if isinstance(file_attr, str):
                        actual_video_path = file_attr
                    else:
                        print(f"video.__file ä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹: {type(file_attr)}")
                elif hasattr(video, 'path'):
                    actual_video_path = video.path
                elif hasattr(video, 'video_path'):
                    actual_video_path = video.video_path
                elif hasattr(video, '_path'):
                    actual_video_path = video._path
                elif hasattr(video, 'file_path'):
                    actual_video_path = video.file_path
                else:
                    # å°è¯•ç›´æ¥ä½¿ç”¨videoä½œä¸ºè·¯å¾„
                    if isinstance(video, str):
                        actual_video_path = video
                    else:
                        print(f"æ— æ³•ä»videoå¯¹è±¡è·å–è·¯å¾„ï¼Œvideoç±»å‹: {type(video)}")
                        print(f"videoå¯¹è±¡å±æ€§: {[attr for attr in dir(video) if not attr.startswith('_')]}")
                        if hasattr(video, '__dict__'):
                            print(f"videoå¯¹è±¡å†…å®¹: {video.__dict__}")
                        
                        # å°è¯•è°ƒç”¨get_componentsæ¥çœ‹æ˜¯å¦èƒ½è·å–ä¿¡æ¯
                        try:
                            if hasattr(video, 'get_components'):
                                components = video.get_components()
                                print(f"video components: {components}")
                        except Exception as e:
                            print(f"è°ƒç”¨get_componentså¤±è´¥: {e}")
            
            # å¦‚æœvideoè¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨video_path
            if actual_video_path is None or not actual_video_path:
                if video_path and video_path.strip():
                    actual_video_path = video_path.strip()
                else:
                    raise ValueError("æœªæä¾›æœ‰æ•ˆçš„è§†é¢‘è¾“å…¥ã€‚è¯·è¿æ¥LoadVideoèŠ‚ç‚¹åˆ°videoè¾“å…¥ï¼Œæˆ–åœ¨video_pathä¸­è¾“å…¥æ–‡ä»¶è·¯å¾„ã€‚")
            
            # éªŒè¯è§†é¢‘æ–‡ä»¶
            if not os.path.exists(actual_video_path):
                raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {actual_video_path}")
            
            # éªŒè¯è§†é¢‘æ ¼å¼
            if not self._is_valid_video_format(actual_video_path):
                raise ValueError("ä¸æ”¯æŒçš„è§†é¢‘æ ¼å¼")
            
            print(f"å¼€å§‹å¤„ç†è§†é¢‘: {actual_video_path}")
            print(f"å‚æ•° - å¸§é—´éš”: {frame_interval}, æœ€å¤§å¸§æ•°: {max_frames}, æ–¹æ³•: {estimation_method}")
            
            # ä½¿ç”¨ä¼°è®¡å™¨å¤„ç†è§†é¢‘
            result = self.estimator.estimate_from_video(
                video_path=actual_video_path,
                frame_interval=frame_interval,
                max_frames=max_frames
            )
            
            if not result["success"]:
                raise RuntimeError(f"ç›¸æœºå‚æ•°ä¼°è®¡å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
            # å‡†å¤‡è¾“å‡º
            intrinsics_json = self._format_intrinsics_output(result["intrinsics"], output_format)
            poses_json = self._format_poses_output(result["poses"], output_format)
            statistics_json = self._format_statistics_output(result["statistics"], result, output_format)
            
            # å¤„ç†å¯è§†åŒ–å›¾åƒ
            if enable_visualization and result.get("trajectory_visualization") is not None:
                trajectory_img = self._prepare_visualization_image(result["trajectory_visualization"])
            else:
                trajectory_img = self._create_empty_visualization()
            
            print(f"æˆåŠŸå¤„ç† {result['frame_count']} å¸§")
            print(f"ä¼°è®¡çš„ç„¦è·: {result['intrinsics']['focal_length']:.2f}")
            
            return (intrinsics_json, trajectory_img, poses_json, statistics_json)
            
        except Exception as e:
            error_msg = f"è§†é¢‘ç›¸æœºå‚æ•°ä¼°è®¡å‡ºé”™: {str(e)}"
            print(error_msg)
            
            # è¿”å›é”™è¯¯ä¿¡æ¯
            error_json = json.dumps({"error": error_msg, "success": False}, ensure_ascii=False, indent=2)
            empty_img = self._create_empty_visualization()
            
            return (error_json, empty_img, error_json, error_json)

    def _is_valid_video_format(self, video_path: str) -> bool:
        """éªŒè¯è§†é¢‘æ ¼å¼"""
        valid_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv', '.webm']
        ext = os.path.splitext(video_path)[1].lower()
        return ext in valid_extensions

    def _format_intrinsics_output(self, intrinsics: Dict, output_format: str) -> str:
        """æ ¼å¼åŒ–å†…å‚è¾“å‡º"""
        if output_format == "json":
            # ç®€åŒ–ç‰ˆæœ¬ï¼ŒåªåŒ…å«æ ¸å¿ƒå‚æ•°
            simplified = {
                "focal_length": intrinsics["focal_length"],
                "principal_point": intrinsics["principal_point"],
                "image_size": intrinsics["image_size"]
            }
            return json.dumps(simplified, ensure_ascii=False, indent=2)
        else:
            # è¯¦ç»†ç‰ˆæœ¬
            return json.dumps(intrinsics, ensure_ascii=False, indent=2)

    def _format_poses_output(self, poses: List[Dict], output_format: str) -> str:
        """æ ¼å¼åŒ–ä½å§¿è¾“å‡º"""
        if output_format == "json":
            # ç®€åŒ–ç‰ˆæœ¬ï¼ŒåªåŒ…å«ä½ç½®å’Œæ¬§æ‹‰è§’
            simplified_poses = []
            for i, pose in enumerate(poses):
                simplified_poses.append({
                    "frame": i,
                    "position": pose["position"],
                    "rotation_euler": pose["rotation_euler"]
                })
            return json.dumps(simplified_poses, ensure_ascii=False, indent=2)
        else:
            # è¯¦ç»†ç‰ˆæœ¬
            detailed_poses = []
            for i, pose in enumerate(poses):
                detailed_pose = pose.copy()
                detailed_pose["frame"] = i
                detailed_poses.append(detailed_pose)
            return json.dumps(detailed_poses, ensure_ascii=False, indent=2)

    def _format_statistics_output(self, statistics: Dict, result: Dict, output_format: str) -> str:
        """æ ¼å¼åŒ–ç»Ÿè®¡ä¿¡æ¯è¾“å‡º"""
        if output_format == "json":
            # ç®€åŒ–ç‰ˆæœ¬
            simplified_stats = {
                "frame_count": result["frame_count"],
                "total_distance": statistics["total_distance"],
                "success": result["success"]
            }
            return json.dumps(simplified_stats, ensure_ascii=False, indent=2)
        else:
            # è¯¦ç»†ç‰ˆæœ¬
            detailed_stats = statistics.copy()
            detailed_stats.update({
                "frame_count": result["frame_count"],
                "success": result["success"],
                "processing_info": {
                    "total_poses": len(result["poses"]),
                    "has_visualization": result.get("trajectory_visualization") is not None
                }
            })
            return json.dumps(detailed_stats, ensure_ascii=False, indent=2)

    def _prepare_visualization_image(self, trajectory_data) -> torch.Tensor:
        """å‡†å¤‡å¯è§†åŒ–å›¾åƒ"""
        try:
            if isinstance(trajectory_data, list):
                # ä»åˆ—è¡¨è½¬æ¢ä¸ºnumpyæ•°ç»„
                img_array = np.array(trajectory_data, dtype=np.uint8)
            else:
                img_array = trajectory_data
            
            if len(img_array.shape) == 3 and img_array.shape[2] == 3:
                # BGRè½¬RGB
                img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)
                # è½¬æ¢ä¸ºtorch tensor (H, W, C) -> (1, H, W, C)
                img_tensor = torch.from_numpy(img_array.astype(np.float32) / 255.0)
                return img_tensor.unsqueeze(0)
            else:
                return self._create_empty_visualization()
                
        except Exception as e:
            print(f"å¯è§†åŒ–å›¾åƒå¤„ç†é”™è¯¯: {e}")
            return self._create_empty_visualization()

    def _create_empty_visualization(self) -> torch.Tensor:
        """åˆ›å»ºç©ºçš„å¯è§†åŒ–å›¾åƒ"""
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„å ä½å›¾åƒ
        img = np.ones((400, 400, 3), dtype=np.float32) * 0.1  # æ·±ç°è‰²èƒŒæ™¯
        
        # æ·»åŠ æ–‡å­—æç¤º
        try:
            img_uint8 = (img * 255).astype(np.uint8)
            cv2.putText(img_uint8, "No Trajectory", (120, 180), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 2)
            cv2.putText(img_uint8, "Visualization", (110, 220), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 2)
            img = img_uint8.astype(np.float32) / 255.0
        except:
            pass  # å¦‚æœæ·»åŠ æ–‡å­—å¤±è´¥ï¼Œè¿”å›çº¯è‰²å›¾åƒ
        
        return torch.from_numpy(img).unsqueeze(0)

# ç¬¬äºŒä¸ªèŠ‚ç‚¹ï¼šè§†é¢‘å¸§æå–å™¨ï¼ˆè¾…åŠ©èŠ‚ç‚¹ï¼‰
class VideoFrameExtractor:
    """è§†é¢‘å¸§æå–èŠ‚ç‚¹ï¼ˆè¾…åŠ©åŠŸèƒ½ï¼‰"""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "video_path": ("STRING", {
                    "default": "",
                    "tooltip": "è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„"
                }),
                "frame_indices": ("STRING", {
                    "default": "0,10,20,30",
                    "tooltip": "è¦æå–çš„å¸§ç´¢å¼•ï¼Œç”¨é€—å·åˆ†éš”"
                })
            },
            "optional": {
                "resize_width": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 2048,
                    "tooltip": "è°ƒæ•´å›¾åƒå®½åº¦ï¼Œ0è¡¨ç¤ºä¿æŒåŸå°ºå¯¸"
                }),
                "resize_height": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 2048,
                    "tooltip": "è°ƒæ•´å›¾åƒé«˜åº¦ï¼Œ0è¡¨ç¤ºä¿æŒåŸå°ºå¯¸"
                })
            }
        }

    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("extracted_frames", "frame_info")
    OUTPUT_IS_LIST = (True, False)
    FUNCTION = "extract_frames"
    CATEGORY = "ğŸ’ƒVVL/VideoCamera"

    def extract_frames(self, video_path: str, frame_indices: str, resize_width: int = 0, resize_height: int = 0) -> tuple:
        """æå–æŒ‡å®šçš„è§†é¢‘å¸§"""
        try:
            if not os.path.exists(video_path):
                raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            
            # è§£æå¸§ç´¢å¼•
            indices = [int(x.strip()) for x in frame_indices.split(',') if x.strip()]
            
            cap = cv2.VideoCapture(video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            
            extracted_frames = []
            extracted_info = []
            
            for idx in indices:
                if 0 <= idx < total_frames:
                    cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                    ret, frame = cap.read()
                    
                    if ret:
                        # BGRè½¬RGB
                        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                        
                        # è°ƒæ•´å°ºå¯¸
                        if resize_width > 0 and resize_height > 0:
                            frame = cv2.resize(frame, (resize_width, resize_height))
                        
                        # è½¬æ¢ä¸ºtorch tensor
                        frame_tensor = torch.from_numpy(frame.astype(np.float32) / 255.0)
                        extracted_frames.append(frame_tensor)
                        
                        extracted_info.append({
                            "frame_index": idx,
                            "shape": list(frame.shape)
                        })
            
            cap.release()
            
            info_json = json.dumps({
                "total_extracted": len(extracted_frames),
                "video_total_frames": total_frames,
                "frame_details": extracted_info
            }, ensure_ascii=False, indent=2)
            
            return (extracted_frames, info_json)
            
        except Exception as e:
            error_msg = f"å¸§æå–é”™è¯¯: {str(e)}"
            print(error_msg)
            
            # è¿”å›ç©ºå›¾åƒå’Œé”™è¯¯ä¿¡æ¯
            empty_frame = torch.zeros((400, 400, 3), dtype=torch.float32)
            error_json = json.dumps({"error": error_msg}, ensure_ascii=False, indent=2)
            
            return ([empty_frame], error_json)

# èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "VideoCameraEstimator": VideoCameraEstimator,
    "VideoFrameExtractor": VideoFrameExtractor
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "VideoCameraEstimator": "VVL Video Camera Estimator",
    "VideoFrameExtractor": "VVL Video Frame Extractor"
} 